{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
   "metadata": {},
   "source": [
    "# Simple bag-of-words baseline for emotion classification (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
   "metadata": {},
   "source": [
    "Authors: Christine de Kock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this starter notebook, we will take you through the process of emotion classification from text. The notebook was adapted from a notebook for SemEval 2024 Shared Task 1: SemRel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad72575e-4b44-48cd-82d0-ee667cd57af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/angwang/miniforge3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50330823-69db-4204-8646-5847eca34694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dac92ad-3fac-4aa1-aae6-1fe3ad14b1c0",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "The training data consists of a short text and binary labels representing human judgments of the emotions in the text. \n",
    "\n",
    "The data is structured as a CSV file with the following fields:\n",
    "- id: a unique identifier for the sample\n",
    "- text: a sentence or short text\n",
    "- 6 binary fields representing emotion annotations: joy, fear, anger, sadness, surprise\n",
    "\n",
    "The data is multilabel, meaning that more than one of the emotion classes may apply to a given text. \n",
    "\n",
    "Below we will show you how to load and re-format the provided data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4fe8cc2-ba47-4240-bdd2-b1d3ea323134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track_a_00001</td>\n",
       "      <td>But not very happy.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track_a_00002</td>\n",
       "      <td>Well she's not gon na last the whole song like...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track_a_00003</td>\n",
       "      <td>She sat at her Papa's recliner sofa only to mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track_a_00004</td>\n",
       "      <td>Yes, the Oklahoma city bombing.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track_a_00005</td>\n",
       "      <td>They were dancing to Bolero.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0  eng_train_track_a_00001                                But not very happy.   \n",
       "1  eng_train_track_a_00002  Well she's not gon na last the whole song like...   \n",
       "2  eng_train_track_a_00003  She sat at her Papa's recliner sofa only to mo...   \n",
       "3  eng_train_track_a_00004                    Yes, the Oklahoma city bombing.   \n",
       "4  eng_train_track_a_00005                       They were dancing to Bolero.   \n",
       "\n",
       "   Anger  Fear  Joy  Sadness  Surprise  \n",
       "0      0     0    1        1         0  \n",
       "1      0     0    1        0         0  \n",
       "2      0     0    0        0         0  \n",
       "3      1     1    0        1         1  \n",
       "4      0     0    1        0         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and validation data\n",
    "\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "# val = pd.read_csv('public_data/dev/track_a/eng_a.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0767ada-33a9-41d5-965a-3e5d096222a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96e1ed-6cd1-4924-9151-60f3d21a7f4e",
   "metadata": {},
   "source": [
    "## Bag-of-words representation\n",
    "\n",
    "In this tutorial, we use a simple bag-of-words representation to obtain a vector for each text. This vector can then be fed into a machine learning model. More advanced models, including LSTMs and transformers, operate on text directly and to not require the vectorisation step. \n",
    "\n",
    "### Preprocessing \n",
    "We choose to unigrams (that is, individual words) and bigrams (two-word sequences). Texts are lowercased before being vectorised. \n",
    "\n",
    "Further preprocessing steps may include: \n",
    "- stopword removal,\n",
    "- TFIDF normalisation,\n",
    "- lemmatisation / stemming, or\n",
    "- using a different tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12fb9fe2-b593-4bac-a083-64fd4d9b29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train['text'].str.lower()).toarray()\n",
    "# X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "y_train = train[emotions].values\n",
    "# y_val = val[emotions].values\n",
    "\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741f618-f126-4637-b5c7-1e9fb68a3ef3",
   "metadata": {},
   "source": [
    "Finally, we cast the transformed vectors to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f232754-e424-47df-8910-8985e5b89a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "# X_val_t = torch.Tensor(X_val)\n",
    "# y_val_t = torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eae3ce2-592c-4d4f-8e02-a7cf8e464503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_t, y_train_t, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a019d-56e4-4c53-bd85-dfd2308f1bd5",
   "metadata": {},
   "source": [
    "## Characteristics of the data\n",
    "\n",
    "Statistics of the data are printed below. There are 2768 samples in the training data. The input representation consists of 29001 input features and there are 5 output clsees. There is an imbalance in the dataset, with the \"fear\" class being assigned to 58% of samples but the \"anger\" class to only 12%. \n",
    "\n",
    "(Due to the multilabel nature of the data, the percentages do not sum to 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efff5213-7055-48e2-bbf6-22f8cf734b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shape of X: {X_train.shape}')\n",
    "# print(f'Shape of y: {y_train.shape}')\n",
    "# print(f'Number of positives per emotion class:')\n",
    "# _ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4373f821-e149-4ef0-ab2b-f069db413e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2214, 29001])\n",
      "Shape of y: torch.Size([2214, 5])\n",
      "Number of positives per emotion class:\n",
      " - Joy: 674 (24%)\n",
      " - Sadness: 878 (32%)\n",
      " - Surprise: 839 (30%)\n",
      " - Fear: 1611 (58%)\n",
      " - Anger: 333 (12%)\n"
     ]
    }
   ],
   "source": [
    "# Local train split\n",
    "\n",
    "print(f'Shape of X: {X_train_train.shape}')\n",
    "print(f'Shape of y: {y_train_train.shape}')\n",
    "print(f'Number of positives per emotion class:')\n",
    "_ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0b62-3759-4883-a5a8-0e7e41b3c5e8",
   "metadata": {},
   "source": [
    "## Define the model \n",
    "\n",
    "We define a simple neural network model with 1 hidden layer for this task. This can be made arbitrarily more complex, eg. by experimenting with the types of inputs and layers, layer size, depth and regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b5fd614-49e3-4833-8925-e4ce83ff35f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da018344-f73d-4f53-a6d0-64e4548c0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local train train split\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train_train.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45571c77-77d9-442a-ac58-3d7b95e6de4c",
   "metadata": {},
   "source": [
    "## Define the optimisation parameters\n",
    "\n",
    " To perform multilabel classification, we use binary cross entropy with logits. See [here](https://discuss.pytorch.org/t/is-there-an-example-for-multi-class-multilabel-classification-in-pytorch/53579/6) for a motivation. Here, one can explore different optimizers, regularisation levels, learning rates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53d60fa-bb19-4229-9470-19ef42188f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc32f06-bd17-455c-95e4-ea400a7dd8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "614cf6d1-f347-4f8f-a2a5-f1b749031c64",
   "metadata": {},
   "source": [
    "## Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "403e494e-305d-4c7c-ad06-7c09bbbbb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train for a set number of epochs\n",
    "# for epoch in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train_t)\n",
    "#     loss = criterion(output, y_train_t)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49129a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.701\n",
      "Epoch 100: Loss: 0.591\n",
      "Epoch 200: Loss: 0.572\n",
      "Epoch 300: Loss: 0.567\n",
      "Epoch 400: Loss: 0.564\n",
      "Epoch 500: Loss: 0.562\n",
      "Epoch 600: Loss: 0.56\n",
      "Epoch 700: Loss: 0.558\n",
      "Epoch 800: Loss: 0.557\n",
      "Epoch 900: Loss: 0.555\n"
     ]
    }
   ],
   "source": [
    "# Train for a set number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_train)\n",
    "    loss = criterion(output, y_train_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4aab9-a465-4e88-bd42-1a15a30dca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4152b82d-cc7d-42ac-a09d-59e227bbb7cb",
   "metadata": {},
   "source": [
    "## Get class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e5a2b-cc42-4b2b-ae05-ac98820eba14",
   "metadata": {},
   "source": [
    "The model outputs logits to coordinate with the BCE. We use a sigmoid transformation to obtain a score in the range of (0,1). We need to define a classification threshold to obtain a binary prediction from the real-valued model output. This can be determined based on the validation data, and may be different for each emotion. Given the imbalance in the data, we set it slightly lower than 0.5 (the standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b75eb754-c414-44d5-9621-08fa8b068965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(X_val, model, threshold=0.5):\n",
    "#     sig = nn.Sigmoid() \n",
    "#     yhat = sig(model(X_val)).detach().numpy()\n",
    "#     y_pred = yhat > threshold\n",
    "    \n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4840c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Train test split\n",
    "\n",
    "def get_predictions(X_train_test, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_train_test)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54bf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = get_predictions(X_train_test, model, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3654a41-ae1b-4255-b9b2-63fa89d36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "# # print(y_pred)\n",
    "\n",
    "# # Create a DataFrame to save to CSV\n",
    "# val_data_with_pred = pd.DataFrame(y_pred, columns=['Joy', 'Anger', 'Sadness', 'Surprise', 'Fear'])  # Adjust column names as per your features\n",
    "# # val_data_with_pred['True_Label'] = y_test\n",
    "# # val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "# if not 'id' in val_data_with_pred.columns:\n",
    "#     val_data_with_pred['id'] = val['id']\n",
    "#     val_data_with_pred['text'] = val['text']\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred[['id', 'text', 'Joy', 'Anger', 'Sadness', 'Surprise', 'Fear']]\n",
    "\n",
    "# # Save to CSV\n",
    "# val_data_with_pred.to_csv('pred_eng_a.csv', index=False)\n",
    "\n",
    "# print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2adb71c6-ecc9-4e0c-83c7-317c4c5508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test = get_predictions(X_, model, 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211feff-646a-4421-abc2-6ddc45a0d4cc",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60417e-6e9f-4047-81e6-f88e189d15d3",
   "metadata": {},
   "source": [
    "We evaluate the model based on the micro- and macro-averaged F1 score. The former aggregates the metrics at the per-sample level, whereas the latter does it at the per-class level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daf5638e-233f-4ffe-92d0-dce910482d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_val, y_pred):\n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "        print(f'{average.upper()} recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd4b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a89d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25f87015-27ac-4849-a80e-fea14d9289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d8db933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICRO recall: 0.3685, precision: 0.5668, f1: 0.4467\n",
      "MACRO recall: 0.2, precision: 0.1134, f1: 0.1447\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92568bcf-96ac-4cfd-b5d3-cd745c4548a8",
   "metadata": {},
   "source": [
    "The results show that the macro-averaged F1 is much lower than the micro-averaged score. This indicates that the model might be performing poorly on some of the classes. Below, we evaluate each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d24d69d-1b8e-481f-af7f-ed1cb9a691ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_class(y_val, y_pred):\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f'*** {emotion} ***')\n",
    "    \n",
    "        recall = recall_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        precision = precision_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        f1 = f1_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "        print(f'recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c184d9b7-d8f9-44e8-8e6c-99f56f7c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eb58d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Joy ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Sadness ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Surprise ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n",
      "*** Fear ***\n",
      "recall: 1.0, precision: 0.5668, f1: 0.7235\n",
      "\n",
      "*** Anger ***\n",
      "recall: 0.0, precision: 0.0, f1: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_per_class(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b95ad1-a6b0-4a92-b409-41d65e801ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceac1d52-f2dc-43c2-a3da-cd34c782dacb",
   "metadata": {},
   "source": [
    "## Weighing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50959010-9160-4879-83f1-0726dd2ed0bb",
   "metadata": {},
   "source": [
    "We can see that the model performs well on the \"fear\" class, which is the most common, but poorly on all others, classifying all samples as negative. One way to address this is by assigning weights to the different classes to increase the effect of samples from rare classes. For example, the below snippet can be used to calculate weights based on their relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8eafd78-df3c-4ccf-b1e4-75aba7cb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = y_train.sum(axis=0)/y_train.sum()\n",
    "# weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36fa25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = y_train_train.sum(axis=0)/y_train_train.sum()\n",
    "weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed511d0-1dd9-4e01-a640-84aef5732f7b",
   "metadata": {},
   "source": [
    "These weights can then be assigned to the loss function for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f442358-0a91-4c69-add6-82234597aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model \n",
    "# model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )\n",
    "\n",
    "# # Define training parameters\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# # Train for a number of epochs\n",
    "# for epoch in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train_t)\n",
    "#     loss = criterion(output, y_train_t)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# # Get predictions\n",
    "# y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "\n",
    "# # Evaluate\n",
    "# print('\\n\\nEVALUATION\\n')\n",
    "# evaluate(y_val, y_pred)\n",
    "\n",
    "# print('\\nPER CLASS BREAKDOWN\\n')\n",
    "# evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3118f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.881\n",
      "Epoch 100: Loss: 0.865\n",
      "Epoch 200: Loss: 0.858\n",
      "Epoch 300: Loss: 0.851\n",
      "Epoch 400: Loss: 0.844\n",
      "Epoch 500: Loss: 0.838\n",
      "Epoch 600: Loss: 0.832\n",
      "Epoch 700: Loss: 0.824\n",
      "Epoch 800: Loss: 0.817\n",
      "Epoch 900: Loss: 0.808\n",
      "\n",
      "\n",
      "EVALUATION\n",
      "\n",
      "MICRO recall: 0.7324, precision: 0.4486, f1: 0.5564\n",
      "MACRO recall: 0.6027, precision: 0.381, f1: 0.462\n",
      "\n",
      "PER CLASS BREAKDOWN\n",
      "\n",
      "*** Joy ***\n",
      "recall: 0.4104, precision: 0.3198, f1: 0.3595\n",
      "\n",
      "*** Sadness ***\n",
      "recall: 0.6959, precision: 0.3343, f1: 0.4516\n",
      "\n",
      "*** Surprise ***\n",
      "recall: 0.7267, precision: 0.504, f1: 0.5952\n",
      "\n",
      "*** Fear ***\n",
      "recall: 1.0, precision: 0.5668, f1: 0.7235\n",
      "\n",
      "*** Anger ***\n",
      "recall: 0.1803, precision: 0.1803, f1: 0.1803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model \n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train_train.shape[1])\n",
    "        )\n",
    "\n",
    "# Define training parameters\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_train)\n",
    "    loss = criterion(output, y_train_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred_test = get_predictions(X_train_test, model, 0.45)\n",
    "\n",
    "# Evaluate\n",
    "print('\\n\\nEVALUATION\\n')\n",
    "evaluate(y_train_test, y_pred_test)\n",
    "\n",
    "print('\\nPER CLASS BREAKDOWN\\n')\n",
    "evaluate_per_class(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdf267-544a-4a7e-9527-709e9d63c2e6",
   "metadata": {},
   "source": [
    "Using this approach, we can see that the model performs much better on average (and particularly on the less common classes), even though the final training error is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab53d9-5402-49f8-8a4f-9407298a4091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
