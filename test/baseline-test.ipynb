{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d203c2-4edd-41ed-a45d-dc1fc92fa697",
   "metadata": {},
   "source": [
    "# Simple bag-of-words baseline for emotion classification (Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01ed9f-399e-4b8a-b46f-49369a33ee31",
   "metadata": {},
   "source": [
    "Authors: Christine de Kock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this starter notebook, we will take you through the process of emotion classification from text. The notebook was adapted from a notebook for SemEval 2024 Shared Task 1: SemRel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad72575e-4b44-48cd-82d0-ee667cd57af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\agupt\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\agupt\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\agupt\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843cbd77-1b7f-41df-aec8-1d53fe1199c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8e9d6e-9342-43fd-9a0a-1330caf4e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac92ad-3fac-4aa1-aae6-1fe3ad14b1c0",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "The training data consists of a short text and binary labels representing human judgments of the emotions in the text. \n",
    "\n",
    "The data is structured as a CSV file with the following fields:\n",
    "- id: a unique identifier for the sample\n",
    "- text: a sentence or short text\n",
    "- 6 binary fields representing emotion annotations: joy, fear, anger, sadness, surprise\n",
    "\n",
    "The data is multilabel, meaning that more than one of the emotion classes may apply to a given text. \n",
    "\n",
    "Below we will show you how to load and re-format the provided data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4fe8cc2-ba47-4240-bdd2-b1d3ea323134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track_a_00001</td>\n",
       "      <td>But not very happy.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track_a_00002</td>\n",
       "      <td>Well she's not gon na last the whole song like...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track_a_00003</td>\n",
       "      <td>She sat at her Papa's recliner sofa only to mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track_a_00004</td>\n",
       "      <td>Yes, the Oklahoma city bombing.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track_a_00005</td>\n",
       "      <td>They were dancing to Bolero.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0  eng_train_track_a_00001                                But not very happy.   \n",
       "1  eng_train_track_a_00002  Well she's not gon na last the whole song like...   \n",
       "2  eng_train_track_a_00003  She sat at her Papa's recliner sofa only to mo...   \n",
       "3  eng_train_track_a_00004                    Yes, the Oklahoma city bombing.   \n",
       "4  eng_train_track_a_00005                       They were dancing to Bolero.   \n",
       "\n",
       "   Anger  Fear  Joy  Sadness  Surprise  \n",
       "0      0     0    1        1         0  \n",
       "1      0     0    1        0         0  \n",
       "2      0     0    0        0         0  \n",
       "3      1     1    0        1         1  \n",
       "4      0     0    1        0         0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and validation data\n",
    "\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0767ada-33a9-41d5-965a-3e5d096222a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96e1ed-6cd1-4924-9151-60f3d21a7f4e",
   "metadata": {},
   "source": [
    "## Bag-of-words representation\n",
    "\n",
    "In this tutorial, we use a simple bag-of-words representation to obtain a vector for each text. This vector can then be fed into a machine learning model. More advanced models, including LSTMs and transformers, operate on text directly and to not require the vectorisation step. \n",
    "\n",
    "### Preprocessing \n",
    "We choose to unigrams (that is, individual words) and bigrams (two-word sequences). Texts are lowercased before being vectorised. \n",
    "\n",
    "Further preprocessing steps may include: \n",
    "- stopword removal,\n",
    "- TFIDF normalisation,\n",
    "- lemmatisation / stemming, or\n",
    "- using a different tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text, config):\n",
    "    \"\"\" \n",
    "    Performs Different preprocessing operations.\n",
    "\n",
    "    Parameters:\n",
    "    text (string): passes a line of text (assume sentence segmentation has already been done)\n",
    "\n",
    "    Returns:\n",
    "    List[string]: Should return a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\2\", text)\n",
    "        return text\n",
    "        \n",
    "    def tokenize_text(text):\n",
    "        tokens = re.split(r\"\\s+\",text)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    # Separate Punctuation otherwise Remove it\n",
    "    \n",
    "    if config[\"sep_pn\"] and not config[\"rm_pn\"]:\n",
    "        text = separate_punctuation(text)\n",
    "\n",
    "    if config[\"rm_pn\"] and not config[\"sep_pn\"]:\n",
    "        text = remove_punctuation(text)\n",
    "    \n",
    "    # tokenize text\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Apply Lemmatization or Stemming\n",
    "\n",
    "    if config[\"apply_stemming\"]:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config[\"apply_lemmatization\"]:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "\n",
    "    # Generate bigrams, trigrams and quadgrams\n",
    "    if config[\"add_bigrams\"]:\n",
    "        bigrams = generate_ngrams_from_tokens(tokens, 2)\n",
    "        bg = [i + \" \" + j for (i,j) in bigrams]\n",
    "        tokens += bg\n",
    "\n",
    "    # Remove Stop words\n",
    "    \n",
    "    if config[\"rm_sw\"]:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.833 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "1 0.825 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "2 0.866 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "3 0.86 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "4 0.835 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "5 0.827 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "6 0.866 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "7 0.86 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "8 0.832 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "9 0.82 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "10 0.866 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "11 0.86 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "12 0.837 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "13 0.824 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "14 0.866 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "15 0.861 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "16 0.834 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "17 0.823 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "18 0.866 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "19 0.859 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "20 0.834 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "21 0.823 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "22 0.866 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "23 0.86 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "24 0.832 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "25 0.822 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "26 0.866 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "27 0.859 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "28 0.835 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "29 0.824 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "30 0.866 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "31 0.86 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "32 0.832 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "33 0.823 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "34 0.866 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "35 0.86 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "36 0.839 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "37 0.826 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "38 0.866 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "39 0.86 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "40 0.835 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "41 0.822 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "42 0.866 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "43 0.86 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "44 0.835 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "45 0.826 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "46 0.866 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "47 0.86 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "48 0.835 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "49 0.822 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "50 0.867 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "51 0.86 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "52 0.835 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "53 0.827 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "54 0.866 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "55 0.86 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "56 0.837 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': True}\n",
      "57 0.825 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "58 0.866 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': True}\n",
      "59 0.86 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': False, 'rm_sw': False}\n",
      "60 0.84 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': True}\n",
      "61 0.824 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "62 0.867 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': True}\n",
      "63 0.859 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_configurations():\n",
    "    options = [\n",
    "        \"sep_pn\", \"rm_pn\", \"apply_lemmatization\", \"apply_stemming\", \"add_bigrams\", \"rm_sw\"\n",
    "    ]\n",
    "    \n",
    "    # Generate all combinations of True/False for each option\n",
    "    combinations = itertools.product([True, False], repeat=len(options))\n",
    "    \n",
    "    configurations = []\n",
    "    \n",
    "    # Create a dictionary for each combination\n",
    "    for combo in combinations:\n",
    "        config = {options[i]: combo[i] for i in range(len(options))}\n",
    "        configurations.append(config)\n",
    "    \n",
    "    return configurations\n",
    "\n",
    "# Example usage\n",
    "results = []\n",
    "configs = grid_configurations()\n",
    "for _i, config in enumerate(configs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform([pre_process(i, config) for i in train[\"text\"]]).toarray()\n",
    "    X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "    emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "    y_train = train[emotions].values\n",
    "    y_val = val[emotions].values\n",
    "\n",
    "    X_train_t = torch.Tensor(X_train)\n",
    "    y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "    X_val_t = torch.Tensor(X_val)\n",
    "    y_val_t = torch.Tensor(y_val)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )\n",
    "\n",
    "    weights = y_train.sum(axis=0)/y_train.sum()\n",
    "    weights = max(weights)/weights\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "    \n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "    loss = None\n",
    "    \n",
    "    # Train for a set number of epochs\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_t)\n",
    "        loss = criterion(output, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    results.append((round(loss.item(),3), _i))\n",
    "    print(_i, round(loss.item(),3), config)\n",
    "print(\"MIN LOSS: \", min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.499 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "1 0.508 {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "2 0.509 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "3 0.51 {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "4 0.503 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "5 0.507 {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}\n",
      "6 0.509 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
      "7 0.56 {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False}\n",
      "MIN LOSS:  (0.499, 0)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "configs = [\n",
    "    {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}, # 18\n",
    "    {'sep_pn': False, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}, # 38\n",
    "    {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False},\n",
    "    {'sep_pn': True, 'rm_pn': True, 'apply_lemmatization': False, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False},\n",
    "    {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False}, # 22\n",
    "    {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': False, 'apply_stemming': False, 'add_bigrams': True, 'rm_sw': False},\n",
    "    {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False},\n",
    "    {'sep_pn': False, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': False, 'add_bigrams': False, 'rm_sw': False},\n",
    "]\n",
    "for _i, config in enumerate(configs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform([pre_process(i, config) for i in train[\"text\"]]).toarray()\n",
    "    X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "    emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "    y_train = train[emotions].values\n",
    "    y_val = val[emotions].values\n",
    "\n",
    "    X_train_t = torch.Tensor(X_train)\n",
    "    y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "    X_val_t = torch.Tensor(X_val)\n",
    "    y_val_t = torch.Tensor(y_val)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )\n",
    "    \n",
    "    # weights = y_train.sum(axis=0)/y_train.sum()\n",
    "    # weights = max(weights)/weights\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "    # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights))\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "    loss = None\n",
    "    \n",
    "    # Train for a set number of epochs\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_t)\n",
    "        loss = criterion(output, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    results.append((round(loss.item(),3), _i))\n",
    "    print(_i, round(loss.item(),3), config)\n",
    "print(\"MIN LOSS: \", min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12fb9fe2-b593-4bac-a083-64fd4d9b29ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "config = {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
    "X_train = vectorizer.fit_transform([pre_process(i, config) for i in train[\"text\"]]).toarray()\n",
    "X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "y_train = train[emotions].values\n",
    "y_val = val[emotions].values\n",
    "\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741f618-f126-4637-b5c7-1e9fb68a3ef3",
   "metadata": {},
   "source": [
    "Finally, we cast the transformed vectors to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f232754-e424-47df-8910-8985e5b89a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "X_val_t = torch.Tensor(X_val)\n",
    "y_val_t = torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7eae3ce2-592c-4d4f-8e02-a7cf8e464503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train_t, y_train_t, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a019d-56e4-4c53-bd85-dfd2308f1bd5",
   "metadata": {},
   "source": [
    "## Characteristics of the data\n",
    "\n",
    "Statistics of the data are printed below. There are 2768 samples in the training data. The input representation consists of 29001 input features and there are 5 output clsees. There is an imbalance in the dataset, with the \"fear\" class being assigned to 58% of samples but the \"anger\" class to only 12%. \n",
    "\n",
    "(Due to the multilabel nature of the data, the percentages do not sum to 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efff5213-7055-48e2-bbf6-22f8cf734b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Shape of X: {X_train.shape}')\n",
    "# print(f'Shape of y: {y_train.shape}')\n",
    "# print(f'Number of positives per emotion class:')\n",
    "# _ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4373f821-e149-4ef0-ab2b-f069db413e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2214, 4255])\n",
      "Shape of y: torch.Size([2214, 5])\n",
      "Number of positives per emotion class:\n",
      " - Joy: 674 (24%)\n",
      " - Sadness: 878 (32%)\n",
      " - Surprise: 839 (30%)\n",
      " - Fear: 1611 (58%)\n",
      " - Anger: 333 (12%)\n"
     ]
    }
   ],
   "source": [
    "# Local train split\n",
    "\n",
    "print(f'Shape of X: {X_train_train.shape}')\n",
    "print(f'Shape of y: {y_train_train.shape}')\n",
    "print(f'Number of positives per emotion class:')\n",
    "_ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e0b62-3759-4883-a5a8-0e7e41b3c5e8",
   "metadata": {},
   "source": [
    "## Define the model \n",
    "\n",
    "We define a simple neural network model with 1 hidden layer for this task. This can be made arbitrarily more complex, eg. by experimenting with the types of inputs and layers, layer size, depth and regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b5fd614-49e3-4833-8925-e4ce83ff35f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da018344-f73d-4f53-a6d0-64e4548c0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local train train split\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train_train.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45571c77-77d9-442a-ac58-3d7b95e6de4c",
   "metadata": {},
   "source": [
    "## Define the optimisation parameters\n",
    "\n",
    " To perform multilabel classification, we use binary cross entropy with logits. See [here](https://discuss.pytorch.org/t/is-there-an-example-for-multi-class-multilabel-classification-in-pytorch/53579/6) for a motivation. Here, one can explore different optimizers, regularisation levels, learning rates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a53d60fa-bb19-4229-9470-19ef42188f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cf6d1-f347-4f8f-a2a5-f1b749031c64",
   "metadata": {},
   "source": [
    "## Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "403e494e-305d-4c7c-ad06-7c09bbbbb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train for a set number of epochs\n",
    "# for epoch in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train_t)\n",
    "#     loss = criterion(output, y_train_t)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49129a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.698\n",
      "Epoch 100: Loss: 0.579\n",
      "Epoch 200: Loss: 0.563\n",
      "Epoch 300: Loss: 0.552\n",
      "Epoch 400: Loss: 0.544\n",
      "Epoch 500: Loss: 0.534\n",
      "Epoch 600: Loss: 0.526\n",
      "Epoch 700: Loss: 0.519\n",
      "Epoch 800: Loss: 0.508\n",
      "Epoch 900: Loss: 0.499\n"
     ]
    }
   ],
   "source": [
    "# Train for a set number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_train)\n",
    "    loss = criterion(output, y_train_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4aab9-a465-4e88-bd42-1a15a30dca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4152b82d-cc7d-42ac-a09d-59e227bbb7cb",
   "metadata": {},
   "source": [
    "## Get class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e5a2b-cc42-4b2b-ae05-ac98820eba14",
   "metadata": {},
   "source": [
    "The model outputs logits to coordinate with the BCE. We use a sigmoid transformation to obtain a score in the range of (0,1). We need to define a classification threshold to obtain a binary prediction from the real-valued model output. This can be determined based on the validation data, and may be different for each emotion. Given the imbalance in the data, we set it slightly lower than 0.5 (the standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b75eb754-c414-44d5-9621-08fa8b068965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_predictions(X_val, model, threshold=0.5):\n",
    "#     sig = nn.Sigmoid() \n",
    "#     yhat = sig(model(X_val)).detach().numpy()\n",
    "#     y_pred = yhat > threshold\n",
    "    \n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4840c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Train test split\n",
    "\n",
    "def get_predictions(X_train_test, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_train_test)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f54bf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = get_predictions(X_train_test, model, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3654a41-ae1b-4255-b9b2-63fa89d36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "# # print(y_pred)\n",
    "\n",
    "# # Create a DataFrame to save to CSV\n",
    "# val_data_with_pred = pd.DataFrame(y_pred, columns=['Joy', 'Anger', 'Sadness', 'Surprise', 'Fear'])  # Adjust column names as per your features\n",
    "# # val_data_with_pred['True_Label'] = y_test\n",
    "# # val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "# if not 'id' in val_data_with_pred.columns:\n",
    "#     val_data_with_pred['id'] = val['id']\n",
    "#     val_data_with_pred['text'] = val['text']\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred[['id', 'text', 'Joy', 'Anger', 'Sadness', 'Surprise', 'Fear']]\n",
    "\n",
    "# # Save to CSV\n",
    "# val_data_with_pred.to_csv('pred_eng_a.csv', index=False)\n",
    "\n",
    "# print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2adb71c6-ecc9-4e0c-83c7-317c4c5508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test = get_predictions(X_, model, 0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211feff-646a-4421-abc2-6ddc45a0d4cc",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60417e-6e9f-4047-81e6-f88e189d15d3",
   "metadata": {},
   "source": [
    "We evaluate the model based on the micro- and macro-averaged F1 score. The former aggregates the metrics at the per-sample level, whereas the latter does it at the per-class level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "daf5638e-233f-4ffe-92d0-dce910482d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_val, y_pred):\n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "        print(f'{average.upper()} recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65a89d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25f87015-27ac-4849-a80e-fea14d9289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d8db933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92568bcf-96ac-4cfd-b5d3-cd745c4548a8",
   "metadata": {},
   "source": [
    "The results show that the macro-averaged F1 is much lower than the micro-averaged score. This indicates that the model might be performing poorly on some of the classes. Below, we evaluate each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d24d69d-1b8e-481f-af7f-ed1cb9a691ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_class(y_val, y_pred):\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f'*** {emotion} ***')\n",
    "    \n",
    "        recall = recall_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        precision = precision_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        f1 = f1_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "        print(f'recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c184d9b7-d8f9-44e8-8e6c-99f56f7c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2eb58d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_per_class(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac1d52-f2dc-43c2-a3da-cd34c782dacb",
   "metadata": {},
   "source": [
    "## Weighing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50959010-9160-4879-83f1-0726dd2ed0bb",
   "metadata": {},
   "source": [
    "We can see that the model performs well on the \"fear\" class, which is the most common, but poorly on all others, classifying all samples as negative. One way to address this is by assigning weights to the different classes to increase the effect of samples from rare classes. For example, the below snippet can be used to calculate weights based on their relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8eafd78-df3c-4ccf-b1e4-75aba7cb25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = y_train.sum(axis=0)/y_train.sum()\n",
    "# weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36fa25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = y_train_train.sum(axis=0)/y_train_train.sum()\n",
    "weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed511d0-1dd9-4e01-a640-84aef5732f7b",
   "metadata": {},
   "source": [
    "These weights can then be assigned to the loss function for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f442358-0a91-4c69-add6-82234597aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define model \n",
    "# model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )\n",
    "\n",
    "# # Define training parameters\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# # Train for a number of epochs\n",
    "# for epoch in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train_t)\n",
    "#     loss = criterion(output, y_train_t)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# # Get predictions\n",
    "# y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "\n",
    "# # Evaluate\n",
    "# print('\\n\\nEVALUATION\\n')\n",
    "# evaluate(y_val, y_pred)\n",
    "\n",
    "# print('\\nPER CLASS BREAKDOWN\\n')\n",
    "# evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3118f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.883\n",
      "Epoch 100: Loss: 0.843\n",
      "Epoch 200: Loss: 0.817\n",
      "Epoch 300: Loss: 0.79\n",
      "Epoch 400: Loss: 0.762\n",
      "Epoch 500: Loss: 0.73\n",
      "Epoch 600: Loss: 0.697\n",
      "Epoch 700: Loss: 0.665\n",
      "Epoch 800: Loss: 0.639\n",
      "Epoch 900: Loss: 0.613\n",
      "\n",
      "\n",
      "EVALUATION\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays use different devices: cpu, cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8124\\1880529767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nEVALUATION\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nPER CLASS BREAKDOWN\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8124\\3925097068.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(y_val, y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'macro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m                     )\n\u001b[0;32m    215\u001b[0m                 ):\n\u001b[1;32m--> 216\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2427\u001b[0m     \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2428\u001b[0m     \"\"\"\n\u001b[1;32m-> 2429\u001b[1;33m     _, r, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   2430\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2431\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"skip_parameter_validation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"samples\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m     MCM = multilabel_confusion_matrix(\n\u001b[0m\u001b[0;32m   1835\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"skip_parameter_validation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattach_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mdevice_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agupt\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36mdevice\u001b[1;34m(remove_none, remove_types, *array_list)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mdevice_other\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_array_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdevice_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdevice_other\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    179\u001b[0m                 \u001b[1;34mf\"Input arrays use different devices: {str(device_)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;34mf\"{str(device_other)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays use different devices: cpu, cpu"
     ]
    }
   ],
   "source": [
    "# Define model \n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train_train.shape[1])\n",
    "        )\n",
    "\n",
    "# Define training parameters\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_train)\n",
    "    loss = criterion(output, y_train_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred_test = get_predictions(X_train_test, model, 0.45)\n",
    "\n",
    "# Evaluate\n",
    "print('\\n\\nEVALUATION\\n')\n",
    "evaluate(y_train_test, y_pred_test)\n",
    "\n",
    "print('\\nPER CLASS BREAKDOWN\\n')\n",
    "evaluate_per_class(y_train_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdf267-544a-4a7e-9527-709e9d63c2e6",
   "metadata": {},
   "source": [
    "Using this approach, we can see that the model performs much better on average (and particularly on the less common classes), even though the final training error is higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
