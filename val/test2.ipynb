{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agupt\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\agupt\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Training Loop:   0%|          | 1/1000 [00:02<44:24,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.5531610250473022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  10%|█         | 101/1000 [04:58<38:14,  2.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 0.015423377975821495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  20%|██        | 201/1000 [10:17<38:37,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss: 0.003945645876228809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  30%|███       | 301/1000 [15:06<33:52,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss: 0.04182000458240509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  38%|███▊      | 376/1000 [20:01<37:39,  3.62s/it]  "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "emotions = ['Joy', 'Sadness', 'Surprise', 'Fear', 'Anger']\n",
    "\n",
    "# Preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())  # Lowercase and tokenize\n",
    "    doc = nlp(\" \".join(tokens))  # POS tagging\n",
    "    tokens += [token.pos_ for token in doc]  # Append POS tags\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train_text = [pre_process(i) for i in train[\"text\"]]\n",
    "X_val_text = [pre_process(i) for i in val[\"text\"]]\n",
    "\n",
    "# Feature Extraction\n",
    "X_train = vectorizer.fit_transform(X_train_text).toarray()\n",
    "X_val = vectorizer.transform(X_val_text).toarray()\n",
    "\n",
    "# POS Encoding\n",
    "train_pos_tags = [[token.pos_ for token in nlp(text)] for text in train[\"text\"]]\n",
    "val_pos_tags = [[token.pos_ for token in nlp(text)] for text in val[\"text\"]]\n",
    "\n",
    "max_length = max(max(len(tags) for tags in train_pos_tags), max(len(tags) for tags in val_pos_tags))\n",
    "train_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in train_pos_tags]\n",
    "val_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in val_pos_tags]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "train_pos_encoded = encoder.fit_transform(train_pos_tags)\n",
    "val_pos_encoded = encoder.transform(val_pos_tags)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = np.concatenate((X_train, train_pos_encoded), axis=1)\n",
    "validation_combined_features = np.concatenate((X_val, val_pos_encoded), axis=1)\n",
    "\n",
    "# Reshape y_train for Logistic Regression/SVM\n",
    "y_train = train[emotions].values\n",
    "y_train_flat = np.argmax(y_train, axis=1)  # Convert multi-hot labels to single-label for classifiers\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(combined_features, y_train_flat)\n",
    "\n",
    "# Generate additional features\n",
    "lr_features = lr.predict_proba(combined_features)\n",
    "val_lr_features = lr.predict_proba(validation_combined_features)\n",
    "\n",
    "# Combine with original features\n",
    "final_train_features = np.concatenate((combined_features, lr_features), axis=1)\n",
    "final_val_features = np.concatenate((validation_combined_features, val_lr_features), axis=1)\n",
    "\n",
    "# Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features.shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, y_train.shape[1])\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "features_tensor = torch.tensor(final_train_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "val_features_tensor = torch.tensor(final_val_features, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val[emotions].values, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "losses = []\n",
    "for epoch in tqdm(range(1000), desc=\"Training Loop\"):\n",
    "    for features, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Output Final Loss\n",
    "print(f\"Final Loss after 1000 epochs: {losses[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
