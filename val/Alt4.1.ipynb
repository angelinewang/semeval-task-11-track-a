{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import BertTokenizer\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2768.000000</td>\n",
       "      <td>2768.000000</td>\n",
       "      <td>2768.000000</td>\n",
       "      <td>2768.000000</td>\n",
       "      <td>2768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.120303</td>\n",
       "      <td>0.582009</td>\n",
       "      <td>0.243497</td>\n",
       "      <td>0.317197</td>\n",
       "      <td>0.303107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.325375</td>\n",
       "      <td>0.493318</td>\n",
       "      <td>0.429270</td>\n",
       "      <td>0.465469</td>\n",
       "      <td>0.459684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anger         Fear          Joy      Sadness     Surprise\n",
       "count  2768.000000  2768.000000  2768.000000  2768.000000  2768.000000\n",
       "mean      0.120303     0.582009     0.243497     0.317197     0.303107\n",
       "std       0.325375     0.493318     0.429270     0.465469     0.459684\n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "50%       0.000000     1.000000     0.000000     0.000000     0.000000\n",
       "75%       0.000000     1.000000     0.000000     1.000000     1.000000\n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "emotions = ['Joy', 'Sadness', 'Surprise', 'Fear', 'Anger']\n",
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2768 entries, 0 to 2767\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        2768 non-null   object\n",
      " 1   text      2768 non-null   object\n",
      " 2   Anger     2768 non-null   int64 \n",
      " 3   Fear      2768 non-null   int64 \n",
      " 4   Joy       2768 non-null   int64 \n",
      " 5   Sadness   2768 non-null   int64 \n",
      " 6   Surprise  2768 non-null   int64 \n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 151.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116 entries, 0 to 115\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        116 non-null    object \n",
      " 1   text      116 non-null    object \n",
      " 2   Anger     0 non-null      float64\n",
      " 3   Fear      0 non-null      float64\n",
      " 4   Joy       0 non-null      float64\n",
      " 5   Sadness   0 non-null      float64\n",
      " 6   Surprise  0 non-null      float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "source": [
    "val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Config\n",
    "config = {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
    "\n",
    "# Preprocessing Functions\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text, config):\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"[.,;:!?'\\\"“”\\(\\)]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return [\" \".join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "    # Apply config options\n",
    "    if config['sep_pn'] and not config['rm_pn']:\n",
    "        text = separate_punctuation(text)\n",
    "    if config['rm_pn'] and not config['sep_pn']:\n",
    "        text = remove_punctuation(text)\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    if config['apply_stemming']:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config['apply_lemmatization']:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "    if config['add_bigrams']:\n",
    "        tokens += generate_ngrams_from_tokens(tokens, 2)\n",
    "    if config['rm_sw']:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess and Extract Features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_text = [pre_process(text, config) for text in train[\"text\"]]\n",
    "val_text = [pre_process(text, config) for text in val[\"text\"]]\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "X_val = vectorizer.transform(val_text).toarray()\n",
    "\n",
    "# POS Tagging\n",
    "def extract_pos_tags(texts):\n",
    "    return [[token.pos_ for token in nlp(text)] for text in texts]\n",
    "\n",
    "train_pos_tags = extract_pos_tags(train[\"text\"])\n",
    "val_pos_tags = extract_pos_tags(val[\"text\"])\n",
    "\n",
    "# POS Encoding\n",
    "max_length = max(max(len(tags) for tags in train_pos_tags), max(len(tags) for tags in val_pos_tags))\n",
    "train_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in train_pos_tags]\n",
    "val_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in val_pos_tags]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "train_pos_encoded = encoder.fit_transform(train_pos_tags)\n",
    "val_pos_encoded = encoder.transform(val_pos_tags)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = np.concatenate((X_train, train_pos_encoded), axis=1)\n",
    "validation_combined_features = np.concatenate((X_val, val_pos_encoded), axis=1)\n",
    "\n",
    "# Logistic Regression for Enhanced Features\n",
    "y_train = train[emotions].values\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(combined_features, np.argmax(y_train, axis=1))\n",
    "\n",
    "lr_features = lr.predict_proba(combined_features)\n",
    "val_lr_features = lr.predict_proba(validation_combined_features)\n",
    "\n",
    "final_train_features = np.concatenate((combined_features, lr_features), axis=1)\n",
    "final_val_features = np.concatenate((validation_combined_features, val_lr_features), axis=1)\n",
    "\n",
    "# Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features.shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, y_train.shape[1])\n",
    ")\n",
    "\n",
    "# # DataLoader\n",
    "# features_tensor = torch.tensor(final_train_features, dtype=torch.float32)\n",
    "# labels_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "# data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# weights = y_train.sum(axis=0)/y_train.sum()\n",
    "# weights = max(weights)/weights\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# # criterion = nn.BCEWithLogitsLoss()\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4) # lr=1e-4\n",
    "\n",
    "# # Training Loop\n",
    "# losses = []\n",
    "# for epoch in tqdm(range(400), desc=\"Training Loop\"):\n",
    "#     for features, labels in data_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(features)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     # if epoch % 100 == 0:\n",
    "#     #     print(f\"Epoch {epoch}: Loss: {round(loss.item(), 3)}\")\n",
    "#     # losses.append(loss.item())\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "#         # Save the trained model's weights for future use.\n",
    "#         torch.save(model.state_dict(), f'./23-12-24/net_epoch_{epoch}.pth')\n",
    "#         print(f\"Saved epoch {epoch} weights to './21-12-24/net_epoch_{epoch}.pth'\")\n",
    "#         losses.append(round(loss.item(),3))\n",
    "#     if epoch == 400:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "#         # Save the trained model's weights for future use.\n",
    "#         torch.save(model.state_dict(), f'./23-12-24/net_epoch_{epoch}.pth')\n",
    "#         print(f\"Saved epoch {epoch} weights to './23-12-24/net_epoch_{epoch}.pth'\")\n",
    "#         losses.append(round(loss.item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Final Loss\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Loss after 400 epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlosses\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# # Final Loss\n",
    "# print(f\"Final Loss after 400 epochs: {losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.37153346e-03, 2.03434673e-04, 1.06744582e-04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.01568564e-02, 3.33009357e-01, 2.31915457e-04],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         2.26728178e-02, 9.27401246e-03, 1.08520154e-03],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         4.98633799e-03, 3.81255445e-01, 1.96406709e-02],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.88115472e-01, 4.96022926e-02, 7.41382697e-03],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         3.32888115e-01, 2.58945858e-03, 1.28381599e-02]]),\n",
       " [0.793, 0.738, 0.781, 0.703])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_val_features, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x240393ce3a0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# eps = [100*i for i in range(0,4)]\n",
    "\n",
    "# plt.plot(eps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# y_pred = get_predictions(torch.Tensor(final_val_features), model, 0.45)\n",
    "# # print(y_pred)\n",
    "\n",
    "# # Create a DataFrame to save to CSV\n",
    "# val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# # val_data_with_pred['True_Label'] = y_test\n",
    "# # val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "# val_data_with_pred['id'] = val['id']\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# # Save to CSV\n",
    "# current_time = datetime.datetime.now()\n",
    "# formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "# val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "# print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([128, 5252]) from checkpoint, the shape in current model is torch.Size([128, 5254]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      6\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./23-12-24/net_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m get_predictions(torch\u001b[38;5;241m.\u001b[39mTensor(final_val_features), model, \u001b[38;5;241m0.45\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(y_pred)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame to save to CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 0.weight: copying a param with shape torch.Size([128, 5252]) from checkpoint, the shape in current model is torch.Size([128, 5254])."
     ]
    }
   ],
   "source": [
    "# # DO 10 different prediction files with epochs 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 Model parameters\n",
    "\n",
    "# # Get 10 differen prediction files\n",
    "\n",
    "# for i in range(4):\n",
    "#     epoch = i*100\n",
    "#     model.load_state_dict(torch.load(f'./23-12-24/net_epoch_{epoch}.pth', weights_only=True))\n",
    "#     y_pred = get_predictions(torch.Tensor(final_val_features), model, 0.45)\n",
    "#     # print(y_pred)\n",
    "\n",
    "#     # Create a DataFrame to save to CSV\n",
    "#     val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "#     # val_data_with_pred['True_Label'] = y_test\n",
    "#     # val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "#     val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "#     val_data_with_pred['id'] = val['id']\n",
    "\n",
    "#     val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "#     # Save to CSV\n",
    "#     current_time = datetime.datetime.now()\n",
    "#     formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "#     val_data_with_pred.to_csv(f'../results/alt4_3/alt4_epoch_{epoch}_pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "#     print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"1.weight\", \"1.bias\", \"1.running_mean\", \"1.running_var\", \"4.weight\", \"4.bias\", \"6.weight\", \"6.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"fc.0.weight\", \"fc.0.bias\", \"fc.1.weight\", \"fc.1.bias\", \"fc.1.running_mean\", \"fc.1.running_var\", \"fc.1.num_batches_tracked\", \"fc.4.weight\", \"fc.4.bias\", \"fc.6.weight\", \"fc.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO 10 different prediction files with epochs 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 Model parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get 10 differen prediction files\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./24-12-24/net_epoch_200.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m get_predictions(torch\u001b[38;5;241m.\u001b[39mTensor(final_val_features), model, \u001b[38;5;241m0.45\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(y_pred)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to save to CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"1.weight\", \"1.bias\", \"1.running_mean\", \"1.running_var\", \"4.weight\", \"4.bias\", \"6.weight\", \"6.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"fc.0.weight\", \"fc.0.bias\", \"fc.1.weight\", \"fc.1.bias\", \"fc.1.running_mean\", \"fc.1.running_var\", \"fc.1.num_batches_tracked\", \"fc.4.weight\", \"fc.4.bias\", \"fc.6.weight\", \"fc.6.bias\". "
     ]
    }
   ],
   "source": [
    "# DO 10 different prediction files with epochs 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 Model parameters\n",
    "\n",
    "# Get 10 differen prediction files\n",
    "\n",
    "model.load_state_dict(torch.load(f'./24-12-24/net_epoch_200.pth', weights_only=True))\n",
    "y_pred = get_predictions(torch.Tensor(final_val_features), model, 0.45)\n",
    "# print(y_pred)\n",
    "\n",
    "# Create a DataFrame to save to CSV\n",
    "val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# val_data_with_pred['True_Label'] = y_test\n",
    "# val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "val_data_with_pred['id'] = val['id']\n",
    "\n",
    "val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/alt4_3/alt4_epoch_{epoch}_pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For Submission 5 files:\n",
    "\n",
    "    Your results for eng track A are:\n",
    "\n",
    "    Multi-label accuracy (Jaccard score): 0.26063218390804593\n",
    "\n",
    "    Micro F1 score: 0.41758241758241754\n",
    "\n",
    "    Macro F1 score: 0.36549662349391404\n",
    "\n",
    "2. For Submission 6 files:\n",
    "\n",
    "    Your results for eng track A are:\n",
    "\n",
    "    Multi-label accuracy (Jaccard score): 0.26206896551724135\n",
    "\n",
    "    Micro F1 score: 0.42659279778393355\n",
    "\n",
    "    Macro F1 score: 0.36911780259117266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For Submission 8 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Epoch | Jaccard Index       | Micro F1            | Macro F1            |\n",
    "| ----- | ------------------- | ------------------- | ------------------- |\n",
    "| 0     | 0.29094827586206906 | 0.4409171075837742  | 0.3952511404048909  |\n",
    "| 100   | 0.24080459770114943 | 0.39560439560439564 | 0.3420526643168152  |\n",
    "| 200   | 0.2548850574712644  | 0.4089635854341737  | 0.3569574389199279  |\n",
    "| 300   | 0.2548850574712644  | 0.41758241758241754 | 0.3576650885346538  |\n",
    "| 400   | 0.2557471264367816  | 0.4098360655737705  | 0.3598377304037682  |\n",
    "| 500   | 0.2530172413793103  | 0.41666666666666663 | 0.35474739097380603 |\n",
    "| 600   | 0.26379310344827583 | 0.41208791208791207 | 0.3499361424130125  |\n",
    "| 700   | 0.27241379310344827 | 0.42622950819672134 | 0.37301587301587297 |\n",
    "| 800   | 0.253448275862069   | 0.4098360655737705  | 0.3496649512518185  |\n",
    "| 900   | 0.25804597701149423 | 0.41961852861035426 | 0.37158938378169115 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
