{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agupt\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\agupt\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Training Loop:   0%|          | 1/1000 [00:02<44:24,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.5531610250473022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  10%|█         | 101/1000 [04:58<38:14,  2.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 0.015423377975821495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  20%|██        | 201/1000 [10:17<38:37,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss: 0.003945645876228809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  30%|███       | 301/1000 [15:06<33:52,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss: 0.04182000458240509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  40%|████      | 401/1000 [21:31<28:22,  2.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Loss: 0.010825814679265022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  50%|█████     | 501/1000 [25:22<18:19,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Loss: 0.0008097641984932125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  60%|██████    | 601/1000 [32:05<29:55,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600: Loss: 0.0012398697435855865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  70%|███████   | 701/1000 [38:30<15:01,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700: Loss: 0.036138493567705154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  80%|████████  | 801/1000 [42:57<08:45,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800: Loss: 0.010699165984988213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  90%|█████████ | 901/1000 [48:01<03:53,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900: Loss: 0.00323837180621922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop: 100%|██████████| 1000/1000 [52:25<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss after 1000 epochs: 0.013590453192591667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "emotions = ['Joy', 'Sadness', 'Surprise', 'Fear', 'Anger']\n",
    "\n",
    "# Preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())  # Lowercase and tokenize\n",
    "    doc = nlp(\" \".join(tokens))  # POS tagging\n",
    "    tokens += [token.pos_ for token in doc]  # Append POS tags\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train_text = [pre_process(i) for i in train[\"text\"]]\n",
    "X_val_text = [pre_process(i) for i in val[\"text\"]]\n",
    "\n",
    "# Feature Extraction\n",
    "X_train = vectorizer.fit_transform(X_train_text).toarray()\n",
    "X_val = vectorizer.transform(X_val_text).toarray()\n",
    "\n",
    "# POS Encoding\n",
    "train_pos_tags = [[token.pos_ for token in nlp(text)] for text in train[\"text\"]]\n",
    "val_pos_tags = [[token.pos_ for token in nlp(text)] for text in val[\"text\"]]\n",
    "\n",
    "max_length = max(max(len(tags) for tags in train_pos_tags), max(len(tags) for tags in val_pos_tags))\n",
    "train_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in train_pos_tags]\n",
    "val_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in val_pos_tags]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "train_pos_encoded = encoder.fit_transform(train_pos_tags)\n",
    "val_pos_encoded = encoder.transform(val_pos_tags)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = np.concatenate((X_train, train_pos_encoded), axis=1)\n",
    "validation_combined_features = np.concatenate((X_val, val_pos_encoded), axis=1)\n",
    "\n",
    "# Reshape y_train for Logistic Regression\n",
    "y_train = train[emotions].values\n",
    "y_train_flat = np.argmax(y_train, axis=1)  # Convert multi-hot labels to single-label for classifiers\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(combined_features, y_train_flat)\n",
    "\n",
    "# Generate additional features\n",
    "lr_features = lr.predict_proba(combined_features)\n",
    "val_lr_features = lr.predict_proba(validation_combined_features)\n",
    "\n",
    "# Combine with original features\n",
    "final_train_features = np.concatenate((combined_features, lr_features), axis=1)\n",
    "final_val_features = np.concatenate((validation_combined_features, val_lr_features), axis=1)\n",
    "\n",
    "# Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features.shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, y_train.shape[1])\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "features_tensor = torch.tensor(final_train_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "val_features_tensor = torch.tensor(final_val_features, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val[emotions].values, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "losses = []\n",
    "for epoch in tqdm(range(1000), desc=\"Training Loop\"):\n",
    "    for features, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# Output Final Loss\n",
    "print(f\"Final Loss after 1000 epochs: {losses[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4611e-02, 4.1625e-03,\n",
       "         2.9160e-04],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8312e-01, 2.9252e-01,\n",
       "         1.3352e-03],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5816e-02, 6.5488e-03,\n",
       "         1.0337e-03],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6322e-02, 2.5584e-01,\n",
       "         5.8693e-03],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8141e-01, 1.4120e-01,\n",
       "         2.7523e-02],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2213e-01, 2.6148e-02,\n",
       "         5.2471e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id  Anger  Fear  Joy  Sadness  Surprise\n",
      "0    eng_dev_track_a_00001      0     1    1        1         0\n",
      "1    eng_dev_track_a_00002      0     0    0        1         0\n",
      "2    eng_dev_track_a_00003      1     0    0        0         0\n",
      "3    eng_dev_track_a_00004      0     0    0        1         0\n",
      "4    eng_dev_track_a_00005      0     1    0        1         0\n",
      "..                     ...    ...   ...  ...      ...       ...\n",
      "111  eng_dev_track_a_00112      0     0    1        1         0\n",
      "112  eng_dev_track_a_00113      0     0    1        1         0\n",
      "113  eng_dev_track_a_00114      1     0    0        0         0\n",
      "114  eng_dev_track_a_00115      0     0    0        0         0\n",
      "115  eng_dev_track_a_00116      0     0    1        0         0\n",
      "\n",
      "[116 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = get_predictions(val_features_tensor, model, 0.45)\n",
    "# print(y_pred)\n",
    "\n",
    "# Create a DataFrame to save to CSV\n",
    "val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# val_data_with_pred['True_Label'] = y_test\n",
    "# val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "val_data_with_pred['id'] = val['id']\n",
    "\n",
    "val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
