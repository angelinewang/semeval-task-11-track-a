{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/angwang/miniforge3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/angwang/miniforge3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/angwang/miniforge3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/angwang/miniforge3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/angwang/miniforge3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/angwang/miniforge3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_train_track_a_00001</td>\n",
       "      <td>But not very happy.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_train_track_a_00002</td>\n",
       "      <td>Well she's not gon na last the whole song like...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_train_track_a_00003</td>\n",
       "      <td>She sat at her Papa's recliner sofa only to mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_train_track_a_00004</td>\n",
       "      <td>Yes, the Oklahoma city bombing.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_train_track_a_00005</td>\n",
       "      <td>They were dancing to Bolero.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0  eng_train_track_a_00001                                But not very happy.   \n",
       "1  eng_train_track_a_00002  Well she's not gon na last the whole song like...   \n",
       "2  eng_train_track_a_00003  She sat at her Papa's recliner sofa only to mo...   \n",
       "3  eng_train_track_a_00004                    Yes, the Oklahoma city bombing.   \n",
       "4  eng_train_track_a_00005                       They were dancing to Bolero.   \n",
       "\n",
       "   Anger  Fear  Joy  Sadness  Surprise  \n",
       "0      0     0    1        1         0  \n",
       "1      0     0    1        0         0  \n",
       "2      0     0    0        0         0  \n",
       "3      1     1    0        1         1  \n",
       "4      0     0    1        0         0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and validation data\n",
    "\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tokeniser search\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# SPACY TOKENISER SETUP \n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NLTK TREEBANK TOKENISER SETUP\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# NLTK WORDPUNCT TOKENISER SETUP \n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "# Tokenize and encode the text\n",
    "\n",
    "def pre_process(text, config):\n",
    "    \"\"\" \n",
    "    Performs Different preprocessing operations.\n",
    "\n",
    "    Parameters:\n",
    "    text (string): passes a line of text (assume sentence segmentation has already been done)\n",
    "\n",
    "    Returns:\n",
    "    List[string]: Should return a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\2\", text)\n",
    "        return text\n",
    "        \n",
    "    def tokenize_text(text):\n",
    "        # Original tokeniser:\n",
    "        # tokens = re.split(r\"\\s+\",text)\n",
    "        # tokens = [t.lower() for t in tokens]\n",
    "        \n",
    "        # BERT TOKENISER SETUP\n",
    "        # Encode text returns a BatchEncoding object\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "        # Convert token ids to words\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "        return tokens\n",
    "\n",
    "        # SPACY TOKENISER SETUP\n",
    "        # doc = nlp(text)\n",
    "        # # Collect only the text of each token in the document\n",
    "        # tokens = [token.text for token in doc]\n",
    "\n",
    "        # NLTK TREEBANKWORD TOKENIZER \n",
    "        # tokenizer = TreebankWordTokenizer()\n",
    "        # tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # NLTK WORDPUNCT TOKENIZER\n",
    "        # tokenizer = WordPunctTokenizer()\n",
    "        # tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        # Return the list of tokens\n",
    "        # return tokens\n",
    "\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    # Separate Punctuation otherwise Remove it\n",
    "    \n",
    "    if config[\"sep_pn\"] and not config[\"rm_pn\"]:\n",
    "        text = separate_punctuation(text)\n",
    "\n",
    "    if config[\"rm_pn\"] and not config[\"sep_pn\"]:\n",
    "        text = remove_punctuation(text)\n",
    "    \n",
    "    # tokenize text\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Apply Lemmatization or Stemming\n",
    "\n",
    "    if config[\"apply_stemming\"]:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config[\"apply_lemmatization\"]:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "\n",
    "    # Generate bigrams, trigrams and quadgrams\n",
    "    if config[\"add_bigrams\"]:\n",
    "        bigrams = generate_ngrams_from_tokens(tokens, 2)\n",
    "        bg = [i + \" \" + j for (i,j) in bigrams]\n",
    "        tokens += bg\n",
    "\n",
    "    # Remove Stop words\n",
    "    \n",
    "    if config[\"rm_sw\"]:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# def grid_configurations():\n",
    "#     options = [\n",
    "#         \"sep_pn\", \"rm_pn\", \"apply_lemmatization\", \"apply_stemming\", \"add_bigrams\", \"rm_sw\"\n",
    "#     ]\n",
    "    \n",
    "#     # Generate all combinations of True/False for each option\n",
    "#     combinations = itertools.product([True, False], repeat=len(options))\n",
    "    \n",
    "#     configurations = []\n",
    "    \n",
    "#     # Create a dictionary for each combination\n",
    "#     for combo in combinations:\n",
    "#         config = {options[i]: combo[i] for i in range(len(options))}\n",
    "#         configurations.append(config)\n",
    "    \n",
    "#     return configurations\n",
    "\n",
    "# # Example usage\n",
    "# results = []\n",
    "# configs = grid_configurations()\n",
    "# for _i, config in enumerate(configs):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     X_train = vectorizer.fit_transform([pre_process(i, config) for i in train[\"text\"]]).toarray()\n",
    "#     X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "#     emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "#     y_train = train[emotions].values\n",
    "#     y_val = val[emotions].values\n",
    "\n",
    "#     X_train_t = torch.Tensor(X_train)\n",
    "#     y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "#     X_val_t = torch.Tensor(X_val)\n",
    "#     y_val_t = torch.Tensor(y_val)\n",
    "\n",
    "#     model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )\n",
    "    \n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "#     loss = None\n",
    "#     # Train for a set number of epochs\n",
    "#     for epoch in range(200):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(X_train_t)\n",
    "#         loss = criterion(output, y_train_t)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     results.append((round(loss.item(),3), _i))\n",
    "#     print(_i, round(loss.item(),3), config)\n",
    "# print(\"MIN LOSS: \", min(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(i, pre_process(i, {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False})) for i in train[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Add Tokeniser search\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # Load pre-trained BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize and encode the text\n",
    "\n",
    "# tokens = tokenizer(\"Example text for sentiment analysis\", return_tensors=\"pt\")\n",
    "\n",
    "# `inputs` now contains input IDs and attention masks that can be fed into a BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# doc = nlp(\"Your sample text goes here.\")\n",
    "# tokens = [token.text for token in doc]\n",
    "\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tokenizer = TweetTokenizer()\n",
    "# tokens = tokenizer.tokenize(\"Your sample tweet goes here :) #exciting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id                                               text  \\\n",
      "0    eng_dev_track_a_00001            My mouth fell open `` No, no, no... I..   \n",
      "1    eng_dev_track_a_00002  You can barely make out your daughter's pale f...   \n",
      "2    eng_dev_track_a_00003  But after blinking my eyes for a few times lep...   \n",
      "3    eng_dev_track_a_00004  Slowly rising to my feet I came to the conclus...   \n",
      "4    eng_dev_track_a_00005  I noticed this months after moving in and doin...   \n",
      "..                     ...                                                ...   \n",
      "111  eng_dev_track_a_00112                       \"ARcH stop your progression.   \n",
      "112  eng_dev_track_a_00113        This 'star', starts to move across the sky.   \n",
      "113  eng_dev_track_a_00114                                  and my feet hurt.   \n",
      "114  eng_dev_track_a_00115        so i cried my eyes out and did the drawing.   \n",
      "115  eng_dev_track_a_00116                              They were coal black.   \n",
      "\n",
      "     Anger  Fear  Joy  Sadness  Surprise  \n",
      "0      NaN   NaN  NaN      NaN       NaN  \n",
      "1      NaN   NaN  NaN      NaN       NaN  \n",
      "2      NaN   NaN  NaN      NaN       NaN  \n",
      "3      NaN   NaN  NaN      NaN       NaN  \n",
      "4      NaN   NaN  NaN      NaN       NaN  \n",
      "..     ...   ...  ...      ...       ...  \n",
      "111    NaN   NaN  NaN      NaN       NaN  \n",
      "112    NaN   NaN  NaN      NaN       NaN  \n",
      "113    NaN   NaN  NaN      NaN       NaN  \n",
      "114    NaN   NaN  NaN      NaN       NaN  \n",
      "115    NaN   NaN  NaN      NaN       NaN  \n",
      "\n",
      "[116 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "# Optimal config #18 \n",
    "config = {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
    "X_train = vectorizer.fit_transform([pre_process(i, config) for i in train[\"text\"]]).toarray()\n",
    "X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "y_train = train[emotions].values\n",
    "y_val = val[emotions].values\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "X_val_t = torch.Tensor(X_val)\n",
    "y_val_t = torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2768, 4074)\n",
      "Shape of y: (2768, 5)\n",
      "Number of positives per emotion class:\n",
      " - Joy: 674 (24%)\n",
      " - Sadness: 878 (32%)\n",
      " - Surprise: 839 (30%)\n",
      " - Fear: 1611 (58%)\n",
      " - Anger: 333 (12%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X: {X_train.shape}')\n",
    "print(f'Shape of y: {y_train.shape}')\n",
    "print(f'Number of positives per emotion class:')\n",
    "_ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.701\n",
      "Epoch 100: Loss: 0.565\n",
      "Epoch 200: Loss: 0.552\n",
      "Epoch 300: Loss: 0.543\n",
      "Epoch 400: Loss: 0.536\n",
      "Epoch 500: Loss: 0.53\n",
      "Epoch 600: Loss: 0.522\n",
      "Epoch 700: Loss: 0.516\n",
      "Epoch 800: Loss: 0.507\n",
      "Epoch 900: Loss: 0.499\n"
     ]
    }
   ],
   "source": [
    "# Train for a set number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id  Anger  Fear  Joy  Sadness  Surprise\n",
      "0    eng_dev_track_a_00001      0     1    1        1         0\n",
      "1    eng_dev_track_a_00002      0     0    1        1         0\n",
      "2    eng_dev_track_a_00003      1     0    0        1         0\n",
      "3    eng_dev_track_a_00004      0     0    0        1         0\n",
      "4    eng_dev_track_a_00005      0     1    0        1         0\n",
      "..                     ...    ...   ...  ...      ...       ...\n",
      "111  eng_dev_track_a_00112      0     0    1        1         0\n",
      "112  eng_dev_track_a_00113      0     0    1        1         0\n",
      "113  eng_dev_track_a_00114      0     1    0        1         0\n",
      "114  eng_dev_track_a_00115      0     0    0        1         0\n",
      "115  eng_dev_track_a_00116      0     1    1        1         0\n",
      "\n",
      "[116 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "# print(y_pred)\n",
    "\n",
    "# Create a DataFrame to save to CSV\n",
    "val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# val_data_with_pred['True_Label'] = y_test\n",
    "# val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "val_data_with_pred['id'] = val['id']\n",
    "\n",
    "val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(y_val, y_pred):\n",
    "#     for average in ['micro', 'macro']:\n",
    "#         recall = recall_score(y_val, y_pred, average=average, zero_division=0)\n",
    "#         precision = precision_score(y_val, y_pred, average=average, zero_division=0)\n",
    "#         f1 = f1_score(y_val, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "#         print(f'{average.upper()} recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_val, y_pred) EVALS WON'T WORK HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_per_class(y_val, y_pred):\n",
    "#     for i, emotion in enumerate(emotions):\n",
    "#         print(f'*** {emotion} ***')\n",
    "    \n",
    "#         recall = recall_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "#         precision = precision_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "#         f1 = f1_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "#         print(f'recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_per_class(y_val, y_pred) EVALS WON'T WORK HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = y_train.sum(axis=0)/y_train.sum()\n",
    "# weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.882\n",
      "Epoch 100: Loss: 0.834\n",
      "Epoch 200: Loss: 0.803\n",
      "Epoch 300: Loss: 0.765\n",
      "Epoch 400: Loss: 0.723\n",
      "Epoch 500: Loss: 0.676\n",
      "Epoch 600: Loss: 0.63\n",
      "Epoch 700: Loss: 0.59\n",
      "Epoch 800: Loss: 0.551\n",
      "Epoch 900: Loss: 0.518\n",
      "                        id  Anger  Fear  Joy  Sadness  Surprise\n",
      "0    eng_dev_track_a_00001      0     1    1        1         0\n",
      "1    eng_dev_track_a_00002      0     0    1        1         0\n",
      "2    eng_dev_track_a_00003      1     0    0        1         0\n",
      "3    eng_dev_track_a_00004      0     1    0        1         0\n",
      "4    eng_dev_track_a_00005      0     0    1        1         0\n",
      "..                     ...    ...   ...  ...      ...       ...\n",
      "111  eng_dev_track_a_00112      0     0    1        1         0\n",
      "112  eng_dev_track_a_00113      0     0    1        1         0\n",
      "113  eng_dev_track_a_00114      0     1    0        1         0\n",
      "114  eng_dev_track_a_00115      0     0    1        1         0\n",
      "115  eng_dev_track_a_00116      0     0    1        1         0\n",
      "\n",
      "[116 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Define model \n",
    "# model = nn.Sequential(\n",
    "#           nn.Linear(X_train.shape[1], 100),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Dropout(0.3),\n",
    "#           nn.Linear(100, y_train.shape[1])\n",
    "#         )\n",
    "\n",
    "# Define training parameters\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# # Train for a number of epochs\n",
    "# for epoch in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(X_train_t)\n",
    "#     loss = criterion(output, y_train_t)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# # Get predictions\n",
    "# y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "\n",
    "# # Create a DataFrame to save to CSV\n",
    "# val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# # val_data_with_pred['True_Label'] = y_test\n",
    "# # val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "# val_data_with_pred['id'] = val['id']\n",
    "\n",
    "# val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "# print(val_data_with_pred)\n",
    "\n",
    "# # Evaluate: EVALS WON'T WORK HERE \n",
    "# # print('\\n\\nEVALUATION\\n')\n",
    "# # evaluate(y_val, y_pred)\n",
    "\n",
    "# # print('\\nPER CLASS BREAKDOWN\\n')\n",
    "# # evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track_a_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track_a_00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track_a_00003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track_a_00004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track_a_00005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track_a_00112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track_a_00113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track_a_00114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track_a_00115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track_a_00116</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  Anger  Fear  Joy  Sadness  Surprise\n",
       "0    eng_dev_track_a_00001      0     1    1        1         0\n",
       "1    eng_dev_track_a_00002      0     0    1        1         0\n",
       "2    eng_dev_track_a_00003      0     1    1        1         0\n",
       "3    eng_dev_track_a_00004      0     1    1        1         0\n",
       "4    eng_dev_track_a_00005      0     1    1        1         0\n",
       "..                     ...    ...   ...  ...      ...       ...\n",
       "111  eng_dev_track_a_00112      0     1    1        1         0\n",
       "112  eng_dev_track_a_00113      0     1    1        1         0\n",
       "113  eng_dev_track_a_00114      0     1    1        1         0\n",
       "114  eng_dev_track_a_00115      0     1    1        1         0\n",
       "115  eng_dev_track_a_00116      0     1    1        1         0\n",
       "\n",
       "[116 rows x 6 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_data_with_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
