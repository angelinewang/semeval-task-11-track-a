{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation data\n",
    "\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \"\"\" \n",
    "    Performs Different preprocessing operations.\n",
    "\n",
    "    Parameters:\n",
    "    text (string): passes a line of text (assume sentence segmentation has already been done)\n",
    "\n",
    "    Returns:\n",
    "    List[string]: Should return a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1\", text)\n",
    "        text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\2\", text)\n",
    "        return text\n",
    "        \n",
    "    def tokenize_text(text):\n",
    "        tokens = re.split(r\"\\s+\",text)\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return stemmed_tokens\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "\n",
    "    # Separate Punctuation otherwise Remove it\n",
    "    \n",
    "    # text = separate_punctuation(text)\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    # tokenize text\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Apply Lemmatization or Stemming\n",
    "\n",
    "    # tokens = apply_stemming(tokens)\n",
    "    tokens = apply_lemmatization(tokens)\n",
    "\n",
    "\n",
    "    # Generate bigrams, trigrams and quadgrams\n",
    "    bigrams = generate_ngrams_from_tokens(tokens, 2)\n",
    "    bg = [i + \" \" + j for (i,j) in bigrams]\n",
    "    tokens += bg\n",
    "\n",
    "    # trigrams = generate_ngrams_from_tokens(tokens, 3)\n",
    "    # tg = [i + \" \" + j + \" \" + k for (i,j,k) in trigrams]\n",
    "    # tokens += tg\n",
    "\n",
    "    # quadgrams = generate_ngrams_from_tokens(tokens, 4)\n",
    "    # qg = [i + \" \" + j + \" \" + k + \" \" + l for (i,j,k,l) in quadgrams]\n",
    "    # tokens += qg\n",
    "\n",
    "    # Remove Stop words\n",
    "\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pre_process(i) for i in train[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform([pre_process(i) for i in train[\"text\"]]).toarray()\n",
    "X_val = vectorizer.transform(val['text'].str.lower()).toarray()\n",
    "\n",
    "emotions = ['Joy','Sadness','Surprise','Fear','Anger']\n",
    "y_train = train[emotions].values\n",
    "y_val = val[emotions].values\n",
    "\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train)\n",
    "y_train_t = torch.Tensor(y_train)\n",
    "\n",
    "X_val_t = torch.Tensor(X_val)\n",
    "y_val_t = torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X: {X_train.shape}')\n",
    "print(f'Shape of y: {y_train.shape}')\n",
    "print(f'Number of positives per emotion class:')\n",
    "_ = [print(f' - {e}: {v} ({round(100*v/len(y_train))}%)') for e,v in zip(emotions, y_train.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a set number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid() \n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "# print(y_pred)\n",
    "\n",
    "# Create a DataFrame to save to CSV\n",
    "val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# val_data_with_pred['True_Label'] = y_test\n",
    "# val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "val_data_with_pred['id'] = val['id']\n",
    "\n",
    "val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_val, y_pred):\n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_val, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "        print(f'{average.upper()} recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(y_val, y_pred) EVALS WON'T WORK HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_class(y_val, y_pred):\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f'*** {emotion} ***')\n",
    "    \n",
    "        recall = recall_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        precision = precision_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        f1 = f1_score(y_val[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "        print(f'recall: {round(recall, 4)}, precision: {round(precision, 4)}, f1: {round(f1, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_per_class(y_val, y_pred) EVALS WON'T WORK HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = y_train.sum(axis=0)/y_train.sum()\n",
    "weights = max(weights)/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model \n",
    "model = nn.Sequential(\n",
    "          nn.Linear(X_train.shape[1], 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(0.3),\n",
    "          nn.Linear(100, y_train.shape[1])\n",
    "        )\n",
    "\n",
    "# Define training parameters\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(weights)) # <-- weights assigned to optimiser\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss: {round(loss.item(),3)}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred = get_predictions(X_val_t, model, 0.45)\n",
    "\n",
    "# Create a DataFrame to save to CSV\n",
    "val_data_with_pred = pd.DataFrame(y_pred, columns=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])  # Adjust column names as per your features\n",
    "# val_data_with_pred['True_Label'] = y_test\n",
    "# val_data_with_pred['Predictions'] = dummy_predictions\n",
    "\n",
    "val_data_with_pred = val_data_with_pred.astype(int)\n",
    "\n",
    "val_data_with_pred['id'] = val['id']\n",
    "\n",
    "val_data_with_pred = val_data_with_pred[['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']]\n",
    "\n",
    "# Save to CSV\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "val_data_with_pred.to_csv(f'../results/pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "print(val_data_with_pred)\n",
    "\n",
    "# Evaluate: EVALS WON'T WORK HERE \n",
    "# print('\\n\\nEVALUATION\\n')\n",
    "# evaluate(y_val, y_pred)\n",
    "\n",
    "# print('\\nPER CLASS BREAKDOWN\\n')\n",
    "# evaluate_per_class(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_with_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
