{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import BertTokenizer\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "emotions = ['Joy', 'Sadness', 'Surprise', 'Fear', 'Anger']\n",
    "emolex_path = \"../EmoLex/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EmoLex Lexicon\n",
    "def load_emolex(emolex_path):\n",
    "    emolex = pd.read_csv(emolex_path, sep='\\t', header=None, names=[\"Word\", \"Emotion\", \"Association\"])\n",
    "    emotion_dict = {}\n",
    "    for _, row in emolex.iterrows():\n",
    "        if row[\"Association\"] == 1:\n",
    "            word = row[\"Word\"]\n",
    "            emotion = row[\"Emotion\"]\n",
    "            if word not in emotion_dict:\n",
    "                emotion_dict[word] = []\n",
    "            emotion_dict[word].append(emotion)\n",
    "    return emotion_dict\n",
    "\n",
    "emotion_dict = load_emolex(emolex_path)\n",
    "\n",
    "# Preprocessing Config\n",
    "config = {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
    "\n",
    "# Preprocessing Functions\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text, config, target_emotion=None, emotion_dict=None):\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?\\'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?\\'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"[.,;:!?\\'\\\"“”\\(\\)]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return [\" \".join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "    # Apply config options\n",
    "    if config['sep_pn'] and not config['rm_pn']:\n",
    "        text = separate_punctuation(text)\n",
    "    if config['rm_pn'] and not config['sep_pn']:\n",
    "        text = remove_punctuation(text)\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    if config['apply_stemming']:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config['apply_lemmatization']:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "    if config['add_bigrams']:\n",
    "        tokens += generate_ngrams_from_tokens(tokens, 2)\n",
    "    if config['rm_sw']:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    if target_emotion and emotion_dict:\n",
    "        relevant_keywords = [word for word in tokens if target_emotion in emotion_dict.get(word, [])]\n",
    "        if relevant_keywords:\n",
    "            processed_text += f\" [SEP] {' '.join(relevant_keywords)}\"\n",
    "        else:\n",
    "            processed_text += \" [SEP]\"\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Preprocess and Extract Features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def preprocess_dataset_with_emotions(dataset, emotions, config, emotion_dict):\n",
    "    augmented_data = {}\n",
    "    for emotion in emotions:\n",
    "        augmented_data[emotion] = [\n",
    "            pre_process(text, config, target_emotion=emotion, emotion_dict=emotion_dict)\n",
    "            for text in dataset\n",
    "        ]\n",
    "    return augmented_data\n",
    "\n",
    "train_augmented = preprocess_dataset_with_emotions(train[\"text\"], emotions, config, emotion_dict)\n",
    "val_augmented = preprocess_dataset_with_emotions(val[\"text\"], emotions, config, emotion_dict)\n",
    "\n",
    "X_train = {emotion: vectorizer.fit_transform(train_augmented[emotion]).toarray() for emotion in emotions}\n",
    "X_val = {emotion: vectorizer.transform(val_augmented[emotion]).toarray() for emotion in emotions}\n",
    "\n",
    "# POS Tagging\n",
    "def extract_pos_tags(texts):\n",
    "    return [[token.pos_ for token in nlp(text)] for text in texts]\n",
    "\n",
    "train_pos_tags = extract_pos_tags(train[\"text\"])\n",
    "val_pos_tags = extract_pos_tags(val[\"text\"])\n",
    "\n",
    "# POS Encoding\n",
    "max_length = max(max(len(tags) for tags in train_pos_tags), max(len(tags) for tags in val_pos_tags))\n",
    "train_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in train_pos_tags]\n",
    "val_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in val_pos_tags]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "train_pos_encoded = encoder.fit_transform(train_pos_tags)\n",
    "val_pos_encoded = encoder.transform(val_pos_tags)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = {\n",
    "    emotion: np.concatenate((X_train[emotion], train_pos_encoded), axis=1) for emotion in emotions\n",
    "}\n",
    "validation_combined_features = {\n",
    "    emotion: np.concatenate((X_val[emotion], val_pos_encoded), axis=1) for emotion in emotions\n",
    "}\n",
    "\n",
    "# Logistic Regression for Enhanced Features\n",
    "y_train = train[emotions].values\n",
    "lr_models = {}\n",
    "lr_features = {}\n",
    "val_lr_features = {}\n",
    "\n",
    "for emotion in emotions:\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(combined_features[emotion], y_train[:, emotions.index(emotion)])\n",
    "    lr_models[emotion] = lr\n",
    "    lr_features[emotion] = lr.predict_proba(combined_features[emotion])\n",
    "    val_lr_features[emotion] = lr.predict_proba(validation_combined_features[emotion])\n",
    "\n",
    "final_train_features = {\n",
    "    emotion: np.concatenate((combined_features[emotion], lr_features[emotion]), axis=1) for emotion in emotions\n",
    "}\n",
    "final_val_features = {\n",
    "    emotion: np.concatenate((validation_combined_features[emotion], val_lr_features[emotion]), axis=1) for emotion in emotions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features[emotions[0]].shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):   2%|▏         | 1/51 [00:01<01:27,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Joy): Loss: 0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  22%|██▏       | 11/51 [00:19<01:12,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Joy): Loss: 1.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  41%|████      | 21/51 [00:40<01:01,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Joy): Loss: 0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  61%|██████    | 31/51 [00:58<00:34,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Joy): Loss: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  80%|████████  | 41/51 [01:17<00:18,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Joy): Loss: 0.751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy): 100%|██████████| 51/51 [01:35<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Joy): Loss: 0.529\n",
      "Epoch 50 (Joy): Loss: 0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):   2%|▏         | 1/51 [00:01<01:14,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Sadness): Loss: 0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  22%|██▏       | 11/51 [00:18<01:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Sadness): Loss: 0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  41%|████      | 21/51 [00:36<00:59,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Sadness): Loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  61%|██████    | 31/51 [00:53<00:36,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Sadness): Loss: 0.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  80%|████████  | 41/51 [01:11<00:17,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Sadness): Loss: 0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness): 100%|██████████| 51/51 [01:25<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Sadness): Loss: 0.252\n",
      "Epoch 50 (Sadness): Loss: 0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):   2%|▏         | 1/51 [00:01<01:01,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Surprise): Loss: 0.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  22%|██▏       | 11/51 [00:09<00:35,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Surprise): Loss: 0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  41%|████      | 21/51 [00:19<00:29,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Surprise): Loss: 0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  61%|██████    | 31/51 [00:28<00:18,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Surprise): Loss: 0.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  80%|████████  | 41/51 [00:37<00:08,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Surprise): Loss: 0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise): 100%|██████████| 51/51 [00:47<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Surprise): Loss: 0.064\n",
      "Epoch 50 (Surprise): Loss: 0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):   2%|▏         | 1/51 [00:00<00:44,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Fear): Loss: 0.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  22%|██▏       | 11/51 [00:10<00:37,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Fear): Loss: 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  41%|████      | 21/51 [00:19<00:25,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Fear): Loss: 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  61%|██████    | 31/51 [00:28<00:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Fear): Loss: 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  80%|████████  | 41/51 [00:37<00:09,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Fear): Loss: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear): 100%|██████████| 51/51 [00:46<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Fear): Loss: 0.037\n",
      "Epoch 50 (Fear): Loss: 0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):   2%|▏         | 1/51 [00:00<00:43,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Anger): Loss: 1.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  22%|██▏       | 11/51 [00:09<00:35,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Anger): Loss: 0.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  41%|████      | 21/51 [00:19<00:28,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Anger): Loss: 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  61%|██████    | 31/51 [00:32<00:25,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Anger): Loss: 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  80%|████████  | 41/51 [00:42<00:10,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Anger): Loss: 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger): 100%|██████████| 51/51 [00:52<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Anger): Loss: 0.017\n",
      "Epoch 50 (Anger): Loss: 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "for emotion in emotions:\n",
    "    features_tensor = torch.tensor(final_train_features[emotion], dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(y_train[:, emotions.index(emotion)], dtype=torch.float32).unsqueeze(1)\n",
    "    dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_count = y_train[:, emotions.index(emotion)].sum()\n",
    "    total_count = y_train.shape[0]\n",
    "    weights = total_count / class_count\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weights], dtype=torch.float32))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Training Loop\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(51), desc=f\"Training Loop ({emotion})\"):\n",
    "        for features, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} ({emotion}): Loss: {round(loss.item(),3)}\")\n",
    "            torch.save(model.state_dict(), f'./17-1-25/{emotion}_net_epoch_{epoch}.pth')\n",
    "            losses.append(round(loss.item(),3))\n",
    "        if epoch == 50:\n",
    "            print(f\"Epoch {epoch} ({emotion}): Loss: {round(loss.item(),3)}\")\n",
    "            torch.save(model.state_dict(), f'./17-1-25/{emotion}_net_epoch_{epoch}.pth')\n",
    "            losses.append(round(loss.item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id    Joy\n",
      "0    eng_dev_track_a_00001   True\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003   True\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005   True\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115   True\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003     True\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113     True\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116    False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116     True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116    False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115    False\n",
      "115  eng_dev_track_a_00116     True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115    False\n",
      "115  eng_dev_track_a_00116    False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115      True\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115      True\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115      True\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115      True\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115     False\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005   True\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003   True\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid()\n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    return y_pred\n",
    "\n",
    "for emotion in emotions:\n",
    "    for i in range(5):\n",
    "        epoch = i*10\n",
    "        model.load_state_dict(torch.load(f'./17-1-25/{emotion}_net_epoch_{epoch}.pth', weights_only=True))\n",
    "        y_pred = get_predictions(torch.Tensor(final_val_features[emotion]), model, 0.45)\n",
    "\n",
    "        val_data_with_pred = pd.DataFrame(y_pred, columns=[emotion])\n",
    "        val_data_with_pred[\"id\"] = val[\"id\"]\n",
    "\n",
    "        val_data_with_pred = val_data_with_pred[[\"id\", emotion]]\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "        val_data_with_pred.to_csv(f'../results/alt4_emolex_1/{emotion}_epoch_{epoch}_pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "        print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "# from datetime import datetime\n",
    "\n",
    "# def evaluate(y_true, y_pred):\n",
    "#     # Calculate Jaccard score\n",
    "#     jaccard = jaccard_score(y_true, y_pred, average='samples')\n",
    "#     print(f'Multilabel accuracy (Jaccard score): {round(jaccard, 4)}')\n",
    "    \n",
    "#     \"\"\"Evaluate with micro and macro metrics for multi-label classification\"\"\"\n",
    "#     for average in ['micro', 'macro']:\n",
    "#         recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "#         precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "#         f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    \n",
    "#         print(f'{average.upper()} recall: {round(recall, 4)}, '\n",
    "#               f'precision: {round(precision, 4)}, '\n",
    "#               f'f1: {round(f1, 4)}')\n",
    "\n",
    "# def evaluate_per_class(y_true, y_pred):\n",
    "#     \"\"\"Evaluate metrics for each emotion separately\"\"\"\n",
    "#     for i, emotion in enumerate(emotions):\n",
    "#         print(f'*** {emotion} ***')\n",
    "    \n",
    "#         recall = recall_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "#         precision = precision_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "#         f1 = f1_score(y_true[:,i], y_pred[:,i], zero_division=0)\n",
    "        \n",
    "#         print(f'recall: {round(recall, 4)}, '\n",
    "#               f'precision: {round(precision, 4)}, '\n",
    "#               f'f1: {round(f1, 4)}\\n')\n",
    "\n",
    "# # After getting predictions, add this evaluation code:\n",
    "# # Evaluate predictions\n",
    "# print(\"\\nEvaluating validation predictions...\")\n",
    "# val_true = val[emotions].values\n",
    "# print(\"Overall Metrics:\")\n",
    "# evaluate(val_true, val_data_with_pred)\n",
    "# print(\"\\nPer-class Metrics:\")\n",
    "# evaluate_per_class(val_true, val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score, recall_score, precision_score, f1_score\n",
    "import os\n",
    "\n",
    "ground_truth = pd.read_csv('../public_data_test/public_data_test/track_a/dev/eng.csv')\n",
    "y_true = ground_truth[[emotion.lower() for emotion in emotions]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_dev_track_a_00001</td>\n",
       "      <td>Older sister (23 at the time) is a Scumbag Stacy.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_dev_track_a_00002</td>\n",
       "      <td>And I laughed like this: garhahagar, because m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_dev_track_a_00003</td>\n",
       "      <td>It overflowed and brown shitty diarrhea water ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_dev_track_a_00004</td>\n",
       "      <td>Its very dark and foggy.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_dev_track_a_00005</td>\n",
       "      <td>Then she tried to, like, have sex with/strangl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>eng_dev_track_a_00112</td>\n",
       "      <td>My heart was beating fast from excitement.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>eng_dev_track_a_00113</td>\n",
       "      <td>A fraying rope stretches down from the rafters.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>eng_dev_track_a_00114</td>\n",
       "      <td>so i cried my eyes out and did the drawing.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>eng_dev_track_a_00115</td>\n",
       "      <td>Never been so close to a group ass-wooping in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>eng_dev_track_a_00116</td>\n",
       "      <td>The song `` Poor Places '' is ironically seepi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0    eng_dev_track_a_00001  Older sister (23 at the time) is a Scumbag Stacy.   \n",
       "1    eng_dev_track_a_00002  And I laughed like this: garhahagar, because m...   \n",
       "2    eng_dev_track_a_00003  It overflowed and brown shitty diarrhea water ...   \n",
       "3    eng_dev_track_a_00004                           Its very dark and foggy.   \n",
       "4    eng_dev_track_a_00005  Then she tried to, like, have sex with/strangl...   \n",
       "..                     ...                                                ...   \n",
       "111  eng_dev_track_a_00112         My heart was beating fast from excitement.   \n",
       "112  eng_dev_track_a_00113    A fraying rope stretches down from the rafters.   \n",
       "113  eng_dev_track_a_00114        so i cried my eyes out and did the drawing.   \n",
       "114  eng_dev_track_a_00115  Never been so close to a group ass-wooping in ...   \n",
       "115  eng_dev_track_a_00116  The song `` Poor Places '' is ironically seepi...   \n",
       "\n",
       "     anger  fear  joy  sadness  surprise  \n",
       "0        1     0    0        0         0  \n",
       "1        0     1    0        0         0  \n",
       "2        1     1    0        1         1  \n",
       "3        0     1    0        0         0  \n",
       "4        1     1    0        0         1  \n",
       "..     ...   ...  ...      ...       ...  \n",
       "111      0     0    1        0         0  \n",
       "112      0     1    0        0         1  \n",
       "113      0     0    0        1         0  \n",
       "114      1     1    0        0         1  \n",
       "115      0     0    1        0         0  \n",
       "\n",
       "[116 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['id', 'Joy'], dtype='object')\n",
      "Unique values in Joy: [ True False]\n",
      "Columns: Index(['id', 'Sadness'], dtype='object')\n",
      "Unique values in Sadness: [ True False]\n",
      "Columns: Index(['id', 'Surprise'], dtype='object')\n",
      "Unique values in Surprise: [False  True]\n",
      "Columns: Index(['id', 'Fear'], dtype='object')\n",
      "Unique values in Fear: [False  True]\n",
      "Columns: Index(['id', 'Anger'], dtype='object')\n",
      "Unique values in Anger: [False  True]\n"
     ]
    }
   ],
   "source": [
    "for i, emotion in enumerate(emotions):\n",
    "    # Use glob to find the file matching the pattern\n",
    "    pattern = os.path.join(results_folder, f\"{emotion}_epoch_{epoch}_*.csv\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "\n",
    "    if matching_files:\n",
    "        # Use the first matching file\n",
    "        file_path = matching_files[0]\n",
    "        emotion_preds = pd.read_csv(file_path)\n",
    "\n",
    "        # Debugging: Check columns and values\n",
    "        print(f\"Columns: {emotion_preds.columns}\")\n",
    "        print(f\"Unique values in {emotion.capitalize()}: {emotion_preds[emotion.capitalize()].unique()}\")\n",
    "\n",
    "        # Handle both 0/1 and True/False cases\n",
    "        emotion_preds[emotion.capitalize()] = (\n",
    "            emotion_preds[emotion.capitalize()]\n",
    "            .astype(str)  # Ensure values are strings for consistency\n",
    "            .str.strip().str.lower()  # Normalize\n",
    "        )\n",
    "\n",
    "        # Map to binary values\n",
    "        y_pred[:, i] = emotion_preds[emotion.capitalize()].map({\n",
    "            'true': 1, 'false': 0, '1': 1, '0': 0\n",
    "        }).fillna(0).astype(int).to_numpy()\n",
    "    else:\n",
    "        print(f\"No file found for pattern: {pattern}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    jaccard = jaccard_score(y_true, y_pred, average='samples')\n",
    "    print(f\"Jaccard score: {round(jaccard, 4)}\")\n",
    "    \n",
    "    for average in ['micro', 'macro']:\n",
    "        recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        print(f\"{average.upper()} - Recall: {round(recall, 4)}, Precision: {round(precision, 4)}, F1: {round(f1, 4)}\")\n",
    "\n",
    "def evaluate_per_class(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate precision, recall, and F1 score per class.\n",
    "    \"\"\"\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        print(f\"*** {emotion.capitalize()} ***\")\n",
    "        # Calculate scores for this emotion (label)\n",
    "        recall = recall_score(y_true[:, i], y_pred[:, i], average=None, zero_division=0)\n",
    "        precision = precision_score(y_true[:, i], y_pred[:, i], average=None, zero_division=0)\n",
    "        f1 = f1_score(y_true[:, i], y_pred[:, i], average=None, zero_division=0)\n",
    "\n",
    "        print(f\"  Recall: {recall}\")\n",
    "        print(f\"  Precision: {precision}\")\n",
    "        print(f\"  F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0 ---\n",
      "Jaccard score: 0.3034\n",
      "MICRO - Recall: 1.0, Precision: 0.3034, F1: 0.4656\n",
      "MACRO - Recall: 1.0, Precision: 0.3034, F1: 0.4507\n",
      "*** Joy ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Sadness ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Surprise ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Fear ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Anger ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "--- Epoch 10 ---\n",
      "Jaccard score: 0.3034\n",
      "MICRO - Recall: 1.0, Precision: 0.3034, F1: 0.4656\n",
      "MACRO - Recall: 1.0, Precision: 0.3034, F1: 0.4507\n",
      "*** Joy ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Sadness ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Surprise ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Fear ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Anger ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "--- Epoch 20 ---\n",
      "Jaccard score: 0.3034\n",
      "MICRO - Recall: 1.0, Precision: 0.3034, F1: 0.4656\n",
      "MACRO - Recall: 1.0, Precision: 0.3034, F1: 0.4507\n",
      "*** Joy ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Sadness ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Surprise ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Fear ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Anger ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "--- Epoch 30 ---\n",
      "Jaccard score: 0.3034\n",
      "MICRO - Recall: 1.0, Precision: 0.3034, F1: 0.4656\n",
      "MACRO - Recall: 1.0, Precision: 0.3034, F1: 0.4507\n",
      "*** Joy ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Sadness ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Surprise ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Fear ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "*** Anger ***\n",
      "  Recall: [0. 0. 0.]\n",
      "  Precision: [0. 0. 0.]\n",
      "  F1 Score: [0. 0. 0.]\n",
      "--- Epoch 40 ---\n",
      "Jaccard score: 0.2799\n",
      "MICRO - Recall: 0.5, Precision: 0.3563, F1: 0.4161\n",
      "MACRO - Recall: 0.4501, Precision: 0.3273, F1: 0.3654\n",
      "*** Joy ***\n",
      "  Recall: [0.25882353 0.70967742]\n",
      "  Precision: [0.70967742 0.25882353]\n",
      "  F1 Score: [0.37931034 0.37931034]\n",
      "*** Sadness ***\n",
      "  Recall: [0.67901235 0.45714286]\n",
      "  Precision: [0.74324324 0.38095238]\n",
      "  F1 Score: [0.70967742 0.41558442]\n",
      "*** Surprise ***\n",
      "  Recall: [0.69411765 0.38709677]\n",
      "  Precision: [0.75641026 0.31578947]\n",
      "  F1 Score: [0.72392638 0.34782609]\n",
      "*** Fear ***\n",
      "  Recall: [0.35849057 0.57142857]\n",
      "  Precision: [0.41304348 0.51428571]\n",
      "  F1 Score: [0.38383838 0.54135338]\n",
      "*** Anger ***\n",
      "  Recall: [0.9   0.125]\n",
      "  Precision: [0.86538462 0.16666667]\n",
      "  F1 Score: [0.88235294 0.14285714]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each epoch\n",
    "for epoch, y_pred in predictions_by_epoch.items():\n",
    "    print(f\"--- Epoch {epoch} ---\")\n",
    "    evaluate(y_true, y_pred)\n",
    "    evaluate_per_class(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
