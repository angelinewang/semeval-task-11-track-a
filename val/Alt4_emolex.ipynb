{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import BertTokenizer\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../public_data/train/track_a/eng.csv')\n",
    "val = pd.read_csv('../public_data/dev/track_a/eng_a.csv')\n",
    "emotions = ['Joy', 'Sadness', 'Surprise', 'Fear', 'Anger']\n",
    "emolex_path = \"../EmoLex/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EmoLex Lexicon\n",
    "def load_emolex(emolex_path):\n",
    "    emolex = pd.read_csv(emolex_path, sep='\\t', header=None, names=[\"Word\", \"Emotion\", \"Association\"])\n",
    "    emotion_dict = {}\n",
    "    for _, row in emolex.iterrows():\n",
    "        if row[\"Association\"] == 1:\n",
    "            word = row[\"Word\"]\n",
    "            emotion = row[\"Emotion\"]\n",
    "            if word not in emotion_dict:\n",
    "                emotion_dict[word] = []\n",
    "            emotion_dict[word].append(emotion)\n",
    "    return emotion_dict\n",
    "\n",
    "emotion_dict = load_emolex(emolex_path)\n",
    "\n",
    "# Preprocessing Config\n",
    "config = {'sep_pn': True, 'rm_pn': False, 'apply_lemmatization': True, 'apply_stemming': True, 'add_bigrams': True, 'rm_sw': False}\n",
    "\n",
    "# Preprocessing Functions\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text, config, target_emotion=None, emotion_dict=None):\n",
    "    def separate_punctuation(text):\n",
    "        text = re.sub(r\"(\\w)([.,;:!?\\'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(r\"([.,;:!?\\'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        text = re.sub(r\"[.,;:!?\\'\\\"“”\\(\\)]\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "        return tokens\n",
    "\n",
    "    def apply_stemming(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def apply_lemmatization(tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def generate_ngrams_from_tokens(tokens, n):\n",
    "        return [\" \".join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "    # Apply config options\n",
    "    if config['sep_pn'] and not config['rm_pn']:\n",
    "        text = separate_punctuation(text)\n",
    "    if config['rm_pn'] and not config['sep_pn']:\n",
    "        text = remove_punctuation(text)\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    if config['apply_stemming']:\n",
    "        tokens = apply_stemming(tokens)\n",
    "    if config['apply_lemmatization']:\n",
    "        tokens = apply_lemmatization(tokens)\n",
    "    if config['add_bigrams']:\n",
    "        tokens += generate_ngrams_from_tokens(tokens, 2)\n",
    "    if config['rm_sw']:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    if target_emotion and emotion_dict:\n",
    "        relevant_keywords = [word for word in tokens if target_emotion in emotion_dict.get(word, [])]\n",
    "        if relevant_keywords:\n",
    "            processed_text += f\" [SEP] {' '.join(relevant_keywords)}\"\n",
    "        else:\n",
    "            processed_text += \" [SEP]\"\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Preprocess and Extract Features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def preprocess_dataset_with_emotions(dataset, emotions, config, emotion_dict):\n",
    "    augmented_data = {}\n",
    "    for emotion in emotions:\n",
    "        augmented_data[emotion] = [\n",
    "            pre_process(text, config, target_emotion=emotion, emotion_dict=emotion_dict)\n",
    "            for text in dataset\n",
    "        ]\n",
    "    return augmented_data\n",
    "\n",
    "train_augmented = preprocess_dataset_with_emotions(train[\"text\"], emotions, config, emotion_dict)\n",
    "val_augmented = preprocess_dataset_with_emotions(val[\"text\"], emotions, config, emotion_dict)\n",
    "\n",
    "X_train = {emotion: vectorizer.fit_transform(train_augmented[emotion]).toarray() for emotion in emotions}\n",
    "X_val = {emotion: vectorizer.transform(val_augmented[emotion]).toarray() for emotion in emotions}\n",
    "\n",
    "# POS Tagging\n",
    "def extract_pos_tags(texts):\n",
    "    return [[token.pos_ for token in nlp(text)] for text in texts]\n",
    "\n",
    "train_pos_tags = extract_pos_tags(train[\"text\"])\n",
    "val_pos_tags = extract_pos_tags(val[\"text\"])\n",
    "\n",
    "# POS Encoding\n",
    "max_length = max(max(len(tags) for tags in train_pos_tags), max(len(tags) for tags in val_pos_tags))\n",
    "train_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in train_pos_tags]\n",
    "val_pos_tags = [tags + ['PAD'] * (max_length - len(tags)) for tags in val_pos_tags]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "train_pos_encoded = encoder.fit_transform(train_pos_tags)\n",
    "val_pos_encoded = encoder.transform(val_pos_tags)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = {\n",
    "    emotion: np.concatenate((X_train[emotion], train_pos_encoded), axis=1) for emotion in emotions\n",
    "}\n",
    "validation_combined_features = {\n",
    "    emotion: np.concatenate((X_val[emotion], val_pos_encoded), axis=1) for emotion in emotions\n",
    "}\n",
    "\n",
    "# Logistic Regression for Enhanced Features\n",
    "y_train = train[emotions].values\n",
    "lr_models = {}\n",
    "lr_features = {}\n",
    "val_lr_features = {}\n",
    "\n",
    "for emotion in emotions:\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(combined_features[emotion], y_train[:, emotions.index(emotion)])\n",
    "    lr_models[emotion] = lr\n",
    "    lr_features[emotion] = lr.predict_proba(combined_features[emotion])\n",
    "    val_lr_features[emotion] = lr.predict_proba(validation_combined_features[emotion])\n",
    "\n",
    "final_train_features = {\n",
    "    emotion: np.concatenate((combined_features[emotion], lr_features[emotion]), axis=1) for emotion in emotions\n",
    "}\n",
    "final_val_features = {\n",
    "    emotion: np.concatenate((validation_combined_features[emotion], val_lr_features[emotion]), axis=1) for emotion in emotions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(final_train_features[emotions[0]].shape[1], 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):   2%|▏         | 1/51 [00:04<03:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Joy): Loss: 0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  22%|██▏       | 11/51 [00:34<01:58,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Joy): Loss: 0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  41%|████      | 21/51 [01:03<01:29,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Joy): Loss: 1.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  61%|██████    | 31/51 [01:36<01:07,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Joy): Loss: 1.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy):  80%|████████  | 41/51 [01:58<00:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Joy): Loss: 1.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Joy): 100%|██████████| 51/51 [02:18<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Joy): Loss: 0.665\n",
      "Epoch 50 (Joy): Loss: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):   2%|▏         | 1/51 [00:02<01:44,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Sadness): Loss: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  22%|██▏       | 11/51 [00:24<01:41,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Sadness): Loss: 0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  41%|████      | 21/51 [00:48<01:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Sadness): Loss: 0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  61%|██████    | 31/51 [01:09<00:40,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Sadness): Loss: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness):  80%|████████  | 41/51 [01:29<00:21,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Sadness): Loss: 0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Sadness): 100%|██████████| 51/51 [01:49<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Sadness): Loss: 0.457\n",
      "Epoch 50 (Sadness): Loss: 0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):   2%|▏         | 1/51 [00:02<01:42,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Surprise): Loss: 0.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  22%|██▏       | 11/51 [00:31<01:59,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Surprise): Loss: 0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  41%|████      | 21/51 [00:50<01:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Surprise): Loss: 0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  61%|██████    | 31/51 [01:15<00:55,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Surprise): Loss: 0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise):  80%|████████  | 41/51 [01:45<00:32,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Surprise): Loss: 0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Surprise): 100%|██████████| 51/51 [02:05<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Surprise): Loss: 0.067\n",
      "Epoch 50 (Surprise): Loss: 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):   2%|▏         | 1/51 [00:01<01:35,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Fear): Loss: 0.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  22%|██▏       | 11/51 [00:21<01:17,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Fear): Loss: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  41%|████      | 21/51 [00:41<01:01,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Fear): Loss: 0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  61%|██████    | 31/51 [01:01<00:39,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Fear): Loss: 0.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear):  80%|████████  | 41/51 [01:29<00:30,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Fear): Loss: 0.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Fear): 100%|██████████| 51/51 [02:01<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Fear): Loss: 0.17\n",
      "Epoch 50 (Fear): Loss: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):   2%|▏         | 1/51 [00:03<02:33,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Anger): Loss: 0.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  22%|██▏       | 11/51 [00:33<02:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 (Anger): Loss: 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  41%|████      | 21/51 [01:05<01:36,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 (Anger): Loss: 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  61%|██████    | 31/51 [01:38<01:06,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 (Anger): Loss: 0.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger):  80%|████████  | 41/51 [02:13<00:34,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 (Anger): Loss: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop (Anger): 100%|██████████| 51/51 [02:45<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (Anger): Loss: 0.05\n",
      "Epoch 50 (Anger): Loss: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "for emotion in emotions:\n",
    "    features_tensor = torch.tensor(final_train_features[emotion], dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(y_train[:, emotions.index(emotion)], dtype=torch.float32).unsqueeze(1)\n",
    "    dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_count = y_train[:, emotions.index(emotion)].sum()\n",
    "    total_count = y_train.shape[0]\n",
    "    weights = total_count / class_count\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weights], dtype=torch.float32))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Training Loop\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(51), desc=f\"Training Loop ({emotion})\"):\n",
    "        for features, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} ({emotion}): Loss: {round(loss.item(),3)}\")\n",
    "            torch.save(model.state_dict(), f'./15-1-25/{emotion}_net_epoch_{epoch}.pth')\n",
    "            losses.append(round(loss.item(),3))\n",
    "        if epoch == 50:\n",
    "            print(f\"Epoch {epoch} ({emotion}): Loss: {round(loss.item(),3)}\")\n",
    "            torch.save(model.state_dict(), f'./15-1-25/{emotion}_net_epoch_{epoch}.pth')\n",
    "            losses.append(round(loss.item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id    Joy\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005   True\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115   True\n",
      "115  eng_dev_track_a_00116   True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Joy\n",
      "0    eng_dev_track_a_00001  True\n",
      "1    eng_dev_track_a_00002  True\n",
      "2    eng_dev_track_a_00003  True\n",
      "3    eng_dev_track_a_00004  True\n",
      "4    eng_dev_track_a_00005  True\n",
      "..                     ...   ...\n",
      "111  eng_dev_track_a_00112  True\n",
      "112  eng_dev_track_a_00113  True\n",
      "113  eng_dev_track_a_00114  True\n",
      "114  eng_dev_track_a_00115  True\n",
      "115  eng_dev_track_a_00116  True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id    Joy\n",
      "0    eng_dev_track_a_00001   True\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003   True\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115   True\n",
      "115  eng_dev_track_a_00116   True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id    Joy\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003   True\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005   True\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002     True\n",
      "2    eng_dev_track_a_00003     True\n",
      "3    eng_dev_track_a_00004     True\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115    False\n",
      "115  eng_dev_track_a_00116    False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004     True\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116     True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004     True\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113     True\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116     True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115    False\n",
      "115  eng_dev_track_a_00116     True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Sadness\n",
      "0    eng_dev_track_a_00001     True\n",
      "1    eng_dev_track_a_00002    False\n",
      "2    eng_dev_track_a_00003    False\n",
      "3    eng_dev_track_a_00004    False\n",
      "4    eng_dev_track_a_00005    False\n",
      "..                     ...      ...\n",
      "111  eng_dev_track_a_00112    False\n",
      "112  eng_dev_track_a_00113    False\n",
      "113  eng_dev_track_a_00114    False\n",
      "114  eng_dev_track_a_00115     True\n",
      "115  eng_dev_track_a_00116    False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001      True\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115     False\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115     False\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115     False\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001     False\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115     False\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Surprise\n",
      "0    eng_dev_track_a_00001      True\n",
      "1    eng_dev_track_a_00002     False\n",
      "2    eng_dev_track_a_00003      True\n",
      "3    eng_dev_track_a_00004     False\n",
      "4    eng_dev_track_a_00005     False\n",
      "..                     ...       ...\n",
      "111  eng_dev_track_a_00112      True\n",
      "112  eng_dev_track_a_00113      True\n",
      "113  eng_dev_track_a_00114     False\n",
      "114  eng_dev_track_a_00115      True\n",
      "115  eng_dev_track_a_00116      True\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112  False\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id   Fear\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002   True\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113   True\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003   True\n",
      "3    eng_dev_track_a_00004   True\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114   True\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001   True\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "                        id  Anger\n",
      "0    eng_dev_track_a_00001  False\n",
      "1    eng_dev_track_a_00002  False\n",
      "2    eng_dev_track_a_00003  False\n",
      "3    eng_dev_track_a_00004  False\n",
      "4    eng_dev_track_a_00005  False\n",
      "..                     ...    ...\n",
      "111  eng_dev_track_a_00112   True\n",
      "112  eng_dev_track_a_00113  False\n",
      "113  eng_dev_track_a_00114  False\n",
      "114  eng_dev_track_a_00115  False\n",
      "115  eng_dev_track_a_00116  False\n",
      "\n",
      "[116 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(X_val, model, threshold=0.5):\n",
    "    sig = nn.Sigmoid()\n",
    "    yhat = sig(model(X_val)).detach().numpy()\n",
    "    y_pred = yhat > threshold\n",
    "    return y_pred\n",
    "\n",
    "for emotion in emotions:\n",
    "    for i in range(5):\n",
    "        epoch = i*10\n",
    "        model.load_state_dict(torch.load(f'./15-1-25/{emotion}_net_epoch_{epoch}.pth', weights_only=True))\n",
    "        y_pred = get_predictions(torch.Tensor(final_val_features[emotion]), model, 0.45)\n",
    "\n",
    "        val_data_with_pred = pd.DataFrame(y_pred, columns=[emotion])\n",
    "        val_data_with_pred[\"id\"] = val[\"id\"]\n",
    "\n",
    "        val_data_with_pred = val_data_with_pred[[\"id\", emotion]]\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        formatted_time = current_time.strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "        val_data_with_pred.to_csv(f'../results/alt4_emolex/{emotion}_epoch_{epoch}_pred_eng_a_{formatted_time}.csv', index=False)\n",
    "\n",
    "        print(val_data_with_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
